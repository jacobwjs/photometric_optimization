{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_path = f'{Path.home()}/SageMaker'\n",
    "print(base_path)\n",
    "%ls $base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(f'{base_path}/photometric_optimization/models')\n",
    "from models.RigNet import RigNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_float_img(x):\n",
    "#     if len(x.size()) > 3:\n",
    "#         x = x.squeeze()\n",
    "    return x.mul_(127.5/255.).add_(0.5)\n",
    "\n",
    "\n",
    "def convert_to_uint8(images, dims=None):\n",
    "    if dims is not None:\n",
    "        images = F.interpolate(images, (256, 256))\n",
    "\n",
    "    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n",
    "    scale = 255 / 2\n",
    "    images = images.mul(scale) \\\n",
    "            .add_(0.5 + scale) \\\n",
    "            .clamp(0, 255) \\\n",
    "            .permute(0, 2, 3, 1) \\\n",
    "            .to('cpu', torch.uint8) \\\n",
    "            .squeeze()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate models with pre-trained weights\n",
    "- DFR regressor\n",
    "- Generator (StyleGAN2)\n",
    "- RigNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load\n",
    "\n",
    "# del dfr/\n",
    "dfr = load.dfr(base_path, load_weights=True, training=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del rignet\n",
    "\n",
    "model_name = './test_results/19-02-2021_10.12.50/ringnet_ckpt_epoch001999.pt'\n",
    "\n",
    "one_hot = True\n",
    "rignet = RigNet(one_hot=one_hot)    \n",
    "# local_path_to_model = f'{base_path}/pretrained_models' \n",
    "# path_to_weights_rignet = f'{local_path_to_model}/{model_name}'\n",
    "path_to_weights_rignet = f'{model_name}'\n",
    "print(\"Loading weights... \", path_to_weights_rignet)\n",
    "rignet.load_state_dict(torch.load(path_to_weights_rignet)['rignet'], strict=True)\n",
    "rignet = rignet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del g_ema\n",
    "g_ema = load.generator(base_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later use in forming images.\n",
    "#\n",
    "w_mean = g_ema.mean_latent(int(5e3)) # For truncation.\n",
    "noise = g_ema.make_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "filename = str(index).zfill(6) + '.pkl'\n",
    "train_dir = f'{base_path}/train_data'\n",
    "A = torch.load(f'{train_dir}/testing_A/{filename}')\n",
    "B = torch.load(f'{train_dir}/testing_B/{filename}')\n",
    "\n",
    "\n",
    "w_A = A['latents'].cuda().unsqueeze(0) # Add in batch dimension\n",
    "landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "images_A = A['images'].cuda()\n",
    "image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "\n",
    "w_B = B['latents'].cuda().unsqueeze(0) # Add in batch dimension\n",
    "landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "images_B = B['images'].cuda()\n",
    "image_masks_B = B['image_masks'].cuda()\n",
    "\n",
    "\n",
    "# print(w_A.shape)\n",
    "# print(w_B.shape)\n",
    "# print(w_A.view(1, -1).shape)\n",
    "p_A = dfr(w_A.view(1, -1))\n",
    "p_B = dfr(w_B.view(1, -1))\n",
    "\n",
    "\n",
    "onehot_pose = F.one_hot(torch.tensor([2]), 6)\n",
    "# labels_in = torch.cat((onehot_pose, onehot_expr), dim=0).cuda()\n",
    "labels_in = onehot_pose.cuda()\n",
    "labels_in = labels_in.expand(1, -1, -1)\n",
    "# print(\"labels_in: \", labels_in)\n",
    "\n",
    "\n",
    "w_AB = rignet(w_A, p_B, labels_in)\n",
    "w_BA = rignet(w_B, p_A, labels_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'truncation_latent': w_mean,\n",
    "    'truncation': 0.75,\n",
    "    'noise': noise,\n",
    "    'randomize_noise': False,\n",
    "    'input_is_latent': True,\n",
    "    'return_latents': False\n",
    "}\n",
    "\n",
    "\n",
    "I_a, _ = g_ema([w_A], **kwargs)\n",
    "I_b, _ = g_ema([w_B], **kwargs)\n",
    "I_ab, _ = g_ema([w_AB], **kwargs)\n",
    "I_ba, _ = g_ema([w_BA], **kwargs)\n",
    "\n",
    "\n",
    "dims = (512, 512)\n",
    "I_a = convert_to_uint8(I_a, dims)\n",
    "I_b = convert_to_uint8(I_b, dims)\n",
    "I_ab = convert_to_uint8(I_ab, dims)\n",
    "I_ba = convert_to_uint8(I_ba, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ab = np.concatenate([I_a, I_b, I_ab], axis=1)\n",
    "res_ba = np.concatenate([I_b, I_a, I_ba], axis=1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(res_ab); plt.axis('off'); plt.show()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(res_ba); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_a = g_ema.get_latent(torch.randn((1, 18, 512)).cuda())\n",
    "W_b = g_ema.get_latent(torch.randn((1, 18, 512)).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # shape, expression, pose, tex, cam, lights = dfr(batch_latents.view(3,-1))\n",
    "# # print(shape.shape)\n",
    "# # print(expression.shape)\n",
    "# # print(pose.shape)\n",
    "# # print(tex.shape)\n",
    "# # print(cam.shape)\n",
    "# # print(lights.shape)\n",
    "\n",
    "P_a = dfr(W_a.view(1,-1))\n",
    "P_b = dfr(W_b.view(1,-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_pose = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "# labels_in = torch.cat((onehot_pose, onehot_expr), dim=0).cuda()\n",
    "labels_in = onehot_pose.cuda()\n",
    "# labels_in = labels_in.expand(args.batch_size, -1, -1)\n",
    "print(\"labels_in: \", labels_in)\n",
    "\n",
    "W_ab = rignet(W_a, P_b, labels_in)\n",
    "W_ba = rignet(W_b, P_a, labels_in)\n",
    "# W_ab = rignet(W_a, P_a, labels_in)\n",
    "# W_ba = rignet(W_b, P_b, labels_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mean = g_ema.mean_latent(int(5e3)) # For truncation.\n",
    "noise = g_ema.make_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "kwargs = {\n",
    "    'truncation_latent': w_mean,\n",
    "    'truncation': 0.75,\n",
    "    'noise': noise,\n",
    "    'randomize_noise': False,\n",
    "    'input_is_latent': True,\n",
    "    'return_latents': False\n",
    "}\n",
    "\n",
    "I_a, _ = g_ema([W_a], **kwargs)\n",
    "I_b, _ = g_ema([W_b], **kwargs)\n",
    "I_ab, _ = g_ema([W_ab], **kwargs)\n",
    "I_ba, _ = g_ema([W_ba], **kwargs)\n",
    "\n",
    "dims = (512, 512)\n",
    "I_a = convert_to_uint8(I_a, dims)\n",
    "I_b = convert_to_uint8(I_b, dims)\n",
    "I_ab = convert_to_uint8(I_ab, dims)\n",
    "I_ba = convert_to_uint8(I_ba, dims)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ab = np.concatenate([I_a, I_b, I_ab], axis=1)\n",
    "res_ba = np.concatenate([I_b, I_a, I_ba], axis=1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(res_ab); plt.axis('off'); plt.show()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(res_ba); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the various RigNet losses\n",
    "**Reconstruction loss**\n",
    "- Ensure can faithfully reproduce latent codes in the training corpus\n",
    "- Want to ensure $RigNet(w,F(w)) = w$\n",
    "- Use the following $l_{2}-loss$ <br>\n",
    "$L_{rec} = ||RigNet(w, F(w)) - w ||_{2}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training corpus for reconstruction loss\n",
    "#\n",
    "batch_size = 1000\n",
    "z = torch.randn((batch_size, 18, 512)).to(device)\n",
    "# with torch.no_grad():\n",
    "#     w_plus = g_ema.get_latent(z)\n",
    "    \n",
    "w_plus = g_ema.get_latent(z).detach()\n",
    "\n",
    "print(w_plus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rignet = RigNet().cuda()\n",
    "dfr = load.dfr(base_path, device)\n",
    "\n",
    "optimizer = Adam(rignet.parameters(), lr=1e-3) # default Adam # Trains\n",
    "# optimizer = Adadelta(rignet.parameters(), lr=0.001) # default Adadelta # Does not train??\n",
    "# optimizer = SGD(rignet.parameters(), lr=.1, momentum=.9) # default SGD # Trains\n",
    "# optimizer = RMSprop(rignet.parameters()) # default RMSprop # Trains\n",
    "# optimizer = ASGD(rignet.parameters()) # default ASGD # Does not train??\n",
    "# optimizer = Adamax(rignet.parameters()) # default Adamax # Trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader to batch up latents in consumable chunks.\n",
    "#\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "loader = DataLoader(w_plus, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "# pbar = tqdm(range(0, epochs), dynamic_ncols=True, smoothing=0.01)\n",
    "pbar = trange(epochs, dynamic_ncols=True, smoothing=0.01)\n",
    "for epoch in pbar:\n",
    "    for step, w in enumerate(loader):\n",
    "        p = dfr(w.view(batch_size, -1))\n",
    "        w_hat = rignet(w, p)\n",
    "\n",
    "        # L2-loss\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        loss = (w_hat - w).abs().square().mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description\n",
    "        (\n",
    "            (\n",
    "                f\"total: {loss:.4f}; recon: {loss:.4f}; \"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cycle-consistent editing loss\n",
    "1. Create small dataset in memory to test\n",
    "    - latents **w** and **v**\n",
    "    - corresponding pose parameters **$p_{w}$** and **$p_{v}$**\n",
    "    - corresponding images **$I_{w}$** and **$I_{v}$**\n",
    "    - landmarks, masks, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam, Adadelta, Adagrad, Adamax, ASGD, LBFGS, RMSprop, Rprop\n",
    "from tqdm.auto import tqdm, trange\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#optimizer = Adam(model.parameters(), lr=1e-3) # default Adam\n",
    "# optimizer = SGD(model.parameters(), lr=.1, momentum=.9) # default SGD\n",
    "#optimizer = SGD(model.parameters(), lr=1e-3, momentum=.9) # original SGD\n",
    "# optimizer = Adadelta(rignet.parameters(), lr=0.01) # default Adadelta\n",
    "#optimizer = Adagrad(model.parameters()) # default Adagrad\n",
    "#optimizer = Adamax(model.parameters()) # default Adamax\n",
    "#optimizer = ASGD(model.parameters()) # default ASGD\n",
    "#optimizer = LBFGS(model.parameters()) # default LBFGS\n",
    "#optimizer = RMSprop(model.parameters()) # default RMSprop\n",
    "#optimizer = Rprop(model.parameters()) # default Rprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# rignet = load.rignet(base_path).cuda()\n",
    "dfr = load.dfr(base_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rignet = load.rignet(base_path, one_hot=True).cuda()\n",
    "# rignet = load.rignet(base_path, one_hot=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO:\n",
    "### - Why must this cell be run multiple times before generator model is loaded?\n",
    "###\n",
    "\n",
    "# sys.path.append(f'{base_path}/stylegan2_pytorch')\n",
    "g_ema = load.generator(base_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "\n",
    "with open(f'{base_path}/photometric_optimization/configs/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "    \n",
    "model_params = config['model_params']\n",
    "path_pretrained = f'{base_path}/pretrained_models'\n",
    "model_params['flame_model_path'] = f'{path_pretrained}/generic_model.pkl'\n",
    "model_params['flame_lmk_embedding_path'] = f'{path_pretrained}/landmark_embedding.npy'\n",
    "model_params['tex_space_path'] = f'{path_pretrained}/FLAME_texture.npz'\n",
    "\n",
    "\n",
    "# del render\n",
    "render = load.renderer(base_path, model_params).cuda()\n",
    "flame = load.flame(base_path, model_params).cuda()\n",
    "flametex = load.flametex(base_path, model_params).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Produce latents\n",
    "# #\n",
    "# batch_size = 4\n",
    "# total_size = batch_size * 1  \n",
    "\n",
    "# w = g_ema.get_latent(torch.randn((total_size, 18, 512)).to(device)).detach()\n",
    "# v = g_ema.get_latent(torch.randn((total_size, 18, 512)).to(device)).detach()\n",
    "\n",
    "# # Produce parameters\n",
    "# #\n",
    "# ### TODO:\n",
    "# ### DFR model should not require inputs to be reshaped. Should be\n",
    "# ### logic inside the model itself.\n",
    "# ###\n",
    "# p_w = dfr(w.view(w.shape[0], -1))\n",
    "# p_v = dfr(v.view(v.shape[0], -1))\n",
    "\n",
    "# # Generate images\n",
    "# #\n",
    "# w_mean = g_ema.mean_latent(4096) # For truncation.\n",
    "# noise = g_ema.make_noise()\n",
    "\n",
    "\n",
    "# kwargs = {\n",
    "#     'truncation_latent': w_mean,\n",
    "#     'truncation': 0.7,\n",
    "#     'noise': noise,\n",
    "#     'randomize_noise': False,\n",
    "#     'input_is_latent': True,\n",
    "#     'return_latents': False\n",
    "# }\n",
    "\n",
    "\n",
    "# # del I_w\n",
    "# # del I_v\n",
    "# I_w, _ = g_ema([w.to(device)], **kwargs)\n",
    "# I_w = I_w.cpu()\n",
    "# I_v, _ = g_ema([v.to(device)], **kwargs)\n",
    "# I_v = I_v.cpu()\n",
    "\n",
    "\n",
    "# # Normalize and interpolate to size expected by flame and landmark models.\n",
    "# #\n",
    "\n",
    "# dims = (224, 224)\n",
    "# I_w = normalize_float_img(I_w)\n",
    "# # I_w = torch.cat(I_w, dim=0)\n",
    "# I_w = F.interpolate(I_w, dims)\n",
    "\n",
    "# I_v = normalize_float_img(I_v)\n",
    "# # I_v = torch.cat(I_v, dim=0)\n",
    "# I_v = F.interpolate(I_v, dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(convert_to_uint8(I_w[0].unsqueeze(0))); plt.axis('off'); plt.show(); \n",
    "# plt.imshow(convert_to_uint8(I_v[0].unsqueeze(0))); plt.axis('off'); plt.show(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, reuse DFR dataset so\n",
    "# don't need to calculate everything.\n",
    "#\n",
    "\n",
    "from torch.utils import data\n",
    "import glob\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "#     batch = list(filter(lambda x: x is not None, batch))\n",
    "#     return torch.utils.data.dataloader.default_collate(batch)\n",
    "    A = [x[0] for x in batch]\n",
    "    B = [x[1] for x in batch]\n",
    "#     return list((A, B)) \n",
    "#     print(\"batch: \", batch)\n",
    "#     print(\"A: \", A)\n",
    "#     print(\"B: \", B)\n",
    "\n",
    "#     print(len(A))\n",
    "#     print(len(B))\n",
    "#     print()\n",
    "    return A, B\n",
    "\n",
    "class DatasetSiamese(data.Dataset):\n",
    "    def __init__(self, path_to_dir_A, path_to_dir_B):\n",
    "        self.path_A = path_to_dir_A\n",
    "        self.path_B = path_to_dir_B\n",
    "        self.files_A = glob.glob1(self.path_A, '*.pkl')\n",
    "        self.files_B = glob.glob1(self.path_B, '*.pkl')\n",
    "        \n",
    "    def __len__(self):\n",
    "        len_A = len(glob.glob1(self.path_A, '*.pkl'))\n",
    "        len_B = len(glob.glob1(self.path_A, '*.pkl'))\n",
    "        assert len_A == len_B\n",
    "        return len_A\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = str(index).zfill(6) + '.pkl'\n",
    "        example_A = torch.load(f'{self.path_A}/{filename}')\n",
    "        example_B = torch.load(f'{self.path_B}/{filename}')\n",
    "        return example_A, example_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir_A = f'{base_path}/train_data/rignet_A'\n",
    "# train_dir_B = f'{base_path}/train_data/rignet_B'\n",
    "train_dir_A = f'{base_path}/train_data/testing_A'\n",
    "train_dir_B = f'{base_path}/train_data/testing_B'\n",
    "\n",
    "print(\"train_dir: \", train_dir_A)\n",
    "print(\"train_dir: \", train_dir_B)\n",
    "dataset = DatasetSiamese(train_dir_A, train_dir_B)\n",
    "\n",
    "batch_size = 4\n",
    "sampler = None\n",
    "\n",
    "loader = data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=(sampler is None),\n",
    "    num_workers=0,\n",
    "#     collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    sampler=sampler,\n",
    "    drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for A, B in loader:\n",
    "    w_A = A['latents'].cuda()\n",
    "    landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "    images_A = A['images'].cuda()\n",
    "    image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "    w_B = B['latents'].cuda()\n",
    "    landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "    images_B = B['images'].cuda()\n",
    "    image_masks_B = B['image_masks'].cuda()\n",
    "#     print(A.keys())\n",
    "#     print(B.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_A = torch.cat([x['latents'].unsqueeze(0) for x in A], dim=0)\n",
    "# landmarks_2d_gt_B = torch.cat([x['landmarks_2d_gt'].unsqueeze(0) for x in B], dim=0)\n",
    "\n",
    "# print(landmarks_2d_gt_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot conditional labels for supporting RigNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example output from DFR model. This gives the index of the label we want.\n",
    "# e.g. shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "#\n",
    "onehot_pose = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "onehot_expr = F.one_hot(torch.tensor([1]), 6).unsqueeze(0)\n",
    "\n",
    "# labels_in = F.one_hot(torch.tensor([4]), 6).unsqueeze(0).expand(4,-1,-1)\n",
    "# labels_in = labels_in.cuda()\n",
    "\n",
    "# labels_in = torch.cat((onehot_pose, onehot_expr), dim=0).cuda()\n",
    "\n",
    "labels_in = onehot_pose\n",
    "\n",
    "labels_in = labels_in.expand(batch_size, -1, -1)\n",
    "labels_in = labels_in.cuda()\n",
    "\n",
    "print(labels_in.shape)\n",
    "print(onehot_pose.shape)\n",
    "print(\"labels_in: \", labels_in)\n",
    "print(\"labels_pose: \", onehot_pose)\n",
    "print(\"labels_expr: \", onehot_expr)\n",
    "print(onehot_pose + onehot_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Calculate parameters from latents\n",
    "#\n",
    "# params_A = dfr(w_A.view(w_A.shape[0], -1))\n",
    "# params_B = dfr(w_B.view(w_B.shape[0], -1))\n",
    "\n",
    "params_A = dfr(nn.Flatten()(w_A))\n",
    "params_B = dfr(nn.Flatten()(w_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Transfer semantic \"style\" (e.g. pose) from one latent to another.\n",
    "#\n",
    "w_Ahat = rignet(w_B, params_A, labels_in) # Transfer semantic params from A to latent B\n",
    "w_Bhat = rignet(w_A, params_B, labels_in) # Transfer semantic params from B to latent A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create the cycle, where transferred paramter to latent, and latent to parameter\n",
    "#    should match.\n",
    "#\n",
    "params_AB = dfr(nn.Flatten()(w_Ahat))\n",
    "params_BA = dfr(nn.Flatten()(w_Bhat))\n",
    "\n",
    "# shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "params_edit_A = [x.clone() for x in params_A]\n",
    "params_edit_B = [x.clone() for x in params_B]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Fix broadcast indexing below to select the correc parameter(s) to swap based on labels\n",
    "\n",
    "e.g.\n",
    "```\n",
    ">>> x = torch.randn(3, 4)\n",
    ">>> x\n",
    "tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n",
    "        [-0.4664,  0.2647, -0.1228, -1.1068],\n",
    "        [-1.1734, -0.6571,  0.7230, -0.6004]])\n",
    ">>> indices = torch.tensor([0, 2])\n",
    ">>> torch.index_select(x, 0, indices)\n",
    "tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n",
    "        [-1.1734, -0.6571,  0.7230, -0.6004]])\n",
    ">>> torch.index_select(x, 1, indices)\n",
    "tensor([[ 0.1427, -0.5414],\n",
    "        [-0.4664, -0.1228],\n",
    "        [-1.1734,  0.7230]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_edit_A[2] = params_AB[2] # Ensure pose is maintained\n",
    "# params_edit_B[2] = params_BA[2] # Ensure pose is maintained\n",
    "# params_edit_A[1].shape\n",
    "for label in labels_in:\n",
    "#     params_edit_A[torch.where(label == 1)].shape\n",
    "#     print(torch.where(label == 1)[-1]\n",
    "    print(torch.nonzero(label))\n",
    "    \n",
    "print(torch.nonzero(labels_in))\n",
    "print(params_edit_A[torch.nonzero(labels_in)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the rendering and return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_editing_loss(epoch, example, param, _flame, _flametex, _render,\n",
    "                       savefolder=None, batch_size_save=4):\n",
    "    import util\n",
    "    \n",
    "    loss_mse = nn.MSELoss().cuda()\n",
    "\n",
    "    latents = example['latents'].cuda()\n",
    "    landmarks_2d_gt = example['landmarks_2d_gt'].cuda()\n",
    "    landmarks_3d_gt = example['landmarks_3d_gt'].cuda()\n",
    "    images = example['images'].cuda()\n",
    "    image_masks = example['image_masks'].cuda()\n",
    "    \n",
    "    \n",
    "    # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "    shape, expression, pose, tex, cam, lights = param\n",
    "    vertices, landmarks2d, landmarks3d = _flame(shape_params=shape,\n",
    "                                               expression_params=expression,\n",
    "                                               pose_params=pose)\n",
    "\n",
    "    # render\n",
    "    #\n",
    "    albedos = _flametex(tex) / 255.\n",
    "    \n",
    "    trans_vertices = util.batch_orth_proj(vertices, cam);\n",
    "    trans_vertices[..., 1:] = - trans_vertices[..., 1:]\n",
    "    landmarks2d = util.batch_orth_proj(landmarks2d, cam);\n",
    "    landmarks2d[..., 1:] = - landmarks2d[..., 1:]\n",
    "    landmarks3d = util.batch_orth_proj(landmarks3d, cam);\n",
    "    landmarks3d[..., 1:] = - landmarks3d[..., 1:]\n",
    "    \n",
    "    losses = {}\n",
    "    losses['landmark_2d'] = util.l2_distance(landmarks2d[:, :, :2],\n",
    "                                              landmarks_2d_gt[:, :, :2]) * 10.0\n",
    "    losses['landmark_3d'] = util.l2_distance(landmarks3d[:, :, :2],\n",
    "                                              landmarks_3d_gt[:, :, :2]) * 10.0\n",
    "    losses['shape_reg'] = (torch.sum(shape ** 2) / 2) * 1e-4\n",
    "    losses['expression_reg'] = (torch.sum(expression ** 2) / 2) * 1e-4\n",
    "    losses['pose_reg'] = (torch.sum(pose ** 2) / 2) * 1e-4 #config.w_pose_reg\n",
    "\n",
    "#     # Regularize learned texture.\n",
    "#     #\n",
    "#     losses['texture_reg'] = loss_mse(albedos, texture_mean.repeat(args.batch_size, 1, 1, 1)) \n",
    "    \n",
    "    \n",
    "    ## render\n",
    "    albedos = _flametex(tex) / 255.\n",
    "    ops = _render(vertices, trans_vertices, albedos, lights)\n",
    "\n",
    "    predicted_images = ops['images']\n",
    "    losses['photometric_texture'] = (image_masks * (predicted_images - images).abs()).mean() \\\n",
    "                                    * 1.0 #config.w_pho\n",
    "\n",
    "\n",
    "    all_loss = 0.\n",
    "    for key in losses.keys():\n",
    "        all_loss = all_loss + losses[key]\n",
    "#         losses_to_plot[key].append(losses[key].item()) # Store for plotting later.\n",
    "\n",
    "\n",
    "    losses['all_loss'] = all_loss\n",
    "#     losses_to_plot['all_loss'].append(losses['all_loss'].item())\n",
    "\n",
    "\n",
    "    if savefolder is not None:\n",
    "        bsize = range(0, batch_size_save)\n",
    "        shape_images = render.render_shape(vertices, trans_vertices, images)\n",
    "        save_rendered_imgs(\n",
    "            savefolder,\n",
    "            epoch,\n",
    "            images[bsize].clone(),\n",
    "            landmarks_2d_gt.clone(),\n",
    "            landmarks2d.clone(),\n",
    "            landmarks3d.clone(),\n",
    "            predicted_images[bsize].detach().cpu().float().clone(),\n",
    "            shape_images[bsize].clone(),\n",
    "            albedos[bsize].clone(),\n",
    "            ops['albedo_images'].detach().cpu().clone()[bsize],                        \n",
    "        )\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_A = cycle_editing_loss(A, params_edit_A, flame, flametex, render)\n",
    "loss_B = cycle_editing_loss(B, params_edit_B, flame, flametex, render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(\n",
    "    rignet.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.0001 # config.e_wd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = loss_A['all_loss'] + loss_B['all_loss']\n",
    "\n",
    "optim.zero_grad()\n",
    "total_loss.backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dfr\n",
    "dfr = load.dfr(base_path, load_weights=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(f'{base_path}/stylegan2_pytorch')\n",
    "g_ema = load.generator(base_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(f'{base_path}/photometric_optimization/configs/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "    \n",
    "model_params = config['model_params']\n",
    "path_pretrained = f'{base_path}/pretrained_models'\n",
    "model_params['flame_model_path'] = f'{path_pretrained}/generic_model.pkl'\n",
    "model_params['flame_lmk_embedding_path'] = f'{path_pretrained}/landmark_embedding.npy'\n",
    "model_params['tex_space_path'] = f'{path_pretrained}/FLAME_texture.npz'\n",
    "\n",
    "\n",
    "# del render\n",
    "render = load.renderer(base_path, model_params).cuda()\n",
    "flame = load.flame(base_path, model_params).cuda()\n",
    "flametex = load.flametex(base_path, model_params).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_reconstruction_loss(rignet, w_A, params_A, w_B, params_B, labels_in, scale_recon=10.0):\n",
    "    loss_recon = 0.0\n",
    "    w_Ahat = rignet(w_A, params_A, labels_in) # Reconsruct self\n",
    "    loss_recon = (w_Ahat - w_A).abs().square().mean() * scale_recon\n",
    "    w_Bhat = rignet(w_B, params_B, labels_in) # Reconsruct self\n",
    "    loss_recon += (w_Bhat - w_B).abs().square().mean() * scale_recon\n",
    "    \n",
    "    return loss_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_editing_loss(epoch, example, param, texture_mean,\n",
    "                       _flame, _flametex, _render):\n",
    "    import util\n",
    "    \n",
    "    loss_mse = nn.MSELoss().cuda()\n",
    "\n",
    "    latents = example['latents'].cuda()\n",
    "    landmarks_2d_gt = example['landmarks_2d_gt'].cuda()\n",
    "    landmarks_3d_gt = example['landmarks_3d_gt'].cuda()\n",
    "    images = example['images'].cuda()\n",
    "    image_masks = example['image_masks'].cuda()\n",
    "    \n",
    "    \n",
    "#     # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "    shape, expression, pose, tex, cam, lights = param\n",
    "#     vertices, landmarks2d, landmarks3d = _flame(shape_params=shape,\n",
    "#                                                expression_params=expression,\n",
    "#                                                pose_params=pose)\n",
    "\n",
    "    \n",
    "    \n",
    "#     trans_vertices = util.batch_orth_proj(vertices, cam);\n",
    "#     trans_vertices[..., 1:] = - trans_vertices[..., 1:]\n",
    "#     landmarks2d = util.batch_orth_proj(landmarks2d, cam);\n",
    "#     landmarks2d[..., 1:] = - landmarks2d[..., 1:]\n",
    "#     landmarks3d = util.batch_orth_proj(landmarks3d, cam);\n",
    "#     landmarks3d[..., 1:] = - landmarks3d[..., 1:]\n",
    "    \n",
    "#     # render\n",
    "#     #\n",
    "#     albedos = _flametex(tex) / 255.\n",
    "#     ops = _render(vertices, trans_vertices, albedos, lights)\n",
    "\n",
    "    render_outputs = render_all(param, _flame, _flametex, _render)\n",
    "    vertices = render_outputs['vertices']\n",
    "    landmarks2d = render_outputs['landmarks2d']\n",
    "    landmarks3d = render_outputs['landmarks3d']\n",
    "    trans_vertices = render_outputs['trans_vertices']\n",
    "    albedos = render_outputs['albedos']\n",
    "    ops = render_outputs['ops']\n",
    "    predicted_images = ops['images']\n",
    "    \n",
    "    losses = {}\n",
    "    losses['landmark_2d'] = util.l2_distance(landmarks2d[:, :, :2],\n",
    "                                              landmarks_2d_gt[:, :, :2]) * 10.0\n",
    "    losses['landmark_3d'] = util.l2_distance(landmarks3d[:, :, :2],\n",
    "                                              landmarks_3d_gt[:, :, :2]) * 10.0\n",
    "    losses['shape_reg'] = (torch.sum(shape ** 2) / 2) * 1e-4\n",
    "    losses['expression_reg'] = (torch.sum(expression ** 2) / 2) * 1e-4\n",
    "    losses['pose_reg'] = (torch.sum(pose ** 2) / 2) * 1e-4 #config.w_pose_reg\n",
    "\n",
    "    # Regularize learned texture.\n",
    "    #\n",
    "    losses['texture_reg'] = loss_mse(albedos, texture_mean.repeat(args.batch_size, 1, 1, 1))     \n",
    "    losses['photometric_texture'] = (image_masks * (predicted_images - images).abs()).mean() \\\n",
    "                                    * 1.0 #config.w_pho\n",
    "\n",
    "\n",
    "    all_loss = 0.\n",
    "    for key in losses.keys():\n",
    "        all_loss = all_loss + losses[key]\n",
    "#         losses_to_plot[key].append(losses[key].item()) # Store for plotting later.\n",
    "\n",
    "\n",
    "    losses['all_loss'] = all_loss\n",
    "#     losses_to_plot['all_loss'].append(losses['all_loss'].item())\n",
    "\n",
    "\n",
    "#     if savefolder is not None:\n",
    "#         bsize = range(0, batch_size_save)\n",
    "#         shape_images = render.render_shape(vertices, trans_vertices, images)\n",
    "#         save_rendered_imgs(\n",
    "#             savefolder,\n",
    "#             epoch,\n",
    "#             images[bsize].clone(),\n",
    "#             landmarks_2d_gt.clone(),\n",
    "#             landmarks2d.clone(),\n",
    "#             landmarks3d.clone(),\n",
    "#             predicted_images[bsize].detach().cpu().float().clone(),\n",
    "#             shape_images[bsize].clone(),\n",
    "#             albedos[bsize].clone(),\n",
    "#             ops['albedo_images'].detach().cpu().clone()[bsize],                        \n",
    "#         )\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_all(param, _flame, _flametex, _render):\n",
    "    \n",
    "    shape, expression, pose, tex, cam, lights = param\n",
    "    vertices, landmarks2d, landmarks3d = _flame(shape_params=shape,\n",
    "                                               expression_params=expression,\n",
    "                                               pose_params=pose)\n",
    "    \n",
    "\n",
    "    trans_vertices = util.batch_orth_proj(vertices, cam);\n",
    "    trans_vertices[..., 1:] = - trans_vertices[..., 1:]\n",
    "    landmarks2d = util.batch_orth_proj(landmarks2d, cam);\n",
    "    landmarks2d[..., 1:] = - landmarks2d[..., 1:]\n",
    "    landmarks3d = util.batch_orth_proj(landmarks3d, cam);\n",
    "    landmarks3d[..., 1:] = - landmarks3d[..., 1:] \n",
    "    \n",
    "    # render\n",
    "    #\n",
    "    albedos = _flametex(tex) / 255.\n",
    "    ops = _render(vertices, trans_vertices, albedos, lights)\n",
    "\n",
    "    \n",
    "    out = {}\n",
    "    out['vertices'] = vertices\n",
    "    out['landmarks2d'] = landmarks2d\n",
    "    out['landmarks3d'] = landmarks3d\n",
    "    out['trans_vertices'] = trans_vertices\n",
    "    out['albedos'] = albedos\n",
    "    out['ops'] = ops\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rendered_imgs(savefolder, \n",
    "                       epoch,\n",
    "                       images,\n",
    "                       landmarks_gt,\n",
    "                       landmarks2d,\n",
    "                       landmarks3d=None,\n",
    "                       predicted_images=None,\n",
    "                       shape_images=None,\n",
    "                       albedos=None,\n",
    "                       albedo_images=None):\n",
    "\n",
    "    import torchvision\n",
    "    import util\n",
    "    import cv2\n",
    "    \n",
    "    if not os.path.exists(savefolder):\n",
    "        os.makedirs(savefolder, exist_ok=True)\n",
    "    \n",
    "    grids = {}\n",
    "    grids['images'] = torchvision.utils.make_grid(images).detach().cpu()\n",
    "    grids['landmarks_gt'] = torchvision.utils.make_grid(\n",
    "        util.tensor_vis_landmarks(images.clone().detach(), landmarks_gt))\n",
    "    grids['landmarks2d'] = torchvision.utils.make_grid(\n",
    "        util.tensor_vis_landmarks(images, landmarks2d))\n",
    "    \n",
    "    if landmarks3d is not None:\n",
    "        grids['landmarks3d'] = torchvision.utils.make_grid(\n",
    "            util.tensor_vis_landmarks(images, landmarks3d))\n",
    "    if albedo_images is not None:\n",
    "        grids['albedoimage'] = torchvision.utils.make_grid(albedo_images)\n",
    "    if predicted_images is not None:\n",
    "        grids['render'] = torchvision.utils.make_grid(predicted_images)\n",
    "    if shape_images is not None:\n",
    "        grids['shape'] = torchvision.utils.make_grid(\n",
    "            F.interpolate(shape_images, [224, 224])).detach().float().cpu()\n",
    "    if albedos is not None:\n",
    "        grids['tex'] = torchvision.utils.make_grid(F.interpolate(albedos, [224, 224])).detach().cpu()\n",
    "\n",
    "    grid = torch.cat(list(grids.values()), 1)\n",
    "    grid_image = (grid.numpy().transpose(1, 2, 0).copy() * 255)[:, :, [2, 1, 0]]\n",
    "    grid_image = np.minimum(np.maximum(grid_image, 0), 255).astype(np.uint8)\n",
    "\n",
    "    cv2.imwrite('{}/{}.jpg'.format(savefolder, str(epoch).zfill(6)), grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rignet\n",
    "# one_hot = True\n",
    "one_hot = True\n",
    "rignet = load.rignet(base_path, one_hot=one_hot).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam, Adadelta, Adagrad, Adamax, ASGD, LBFGS, RMSprop, Rprop\n",
    "\n",
    "\n",
    "# optim = torch.optim.Adam(\n",
    "#     rignet.parameters(),\n",
    "#     lr=1e-3,    \n",
    "# #     weight_decay=0.00001 # config.e_wd\n",
    "# )\n",
    "\n",
    "# optim = Adamax(rignet.parameters()) # default Adamax # Trains\n",
    "optim = Adadelta(rignet.parameters(), lr=0.01) # default Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# import os\n",
    "\n",
    "\n",
    "# epochs = int(10e3)\n",
    "\n",
    "\n",
    "# # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "# # onehot_cam = F.one_hot(torch.tensor([4]), 6).unsqueeze(0)\n",
    "# onehot_pose = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "# # labels_in = torch.cat((onehot_pose, onehot_expr), dim=0).cuda()\n",
    "# labels_in = onehot_pose\n",
    "# labels_in = labels_in.expand(batch_size, -1, -1)\n",
    "# labels_in = labels_in.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# now = datetime.now()\n",
    "# dt_string = now.strftime(\"%d-%m-%Y_%H.%M.%S\")     # dd/mm/YY H:M:S\n",
    "# savefolder = os.path.sep.join(['./test_results', f'{dt_string}'])\n",
    "# if not os.path.exists(savefolder):\n",
    "#     os.makedirs(savefolder, exist_ok=True)\n",
    "    \n",
    "# train_dir = f'{base_path}/train_data'\n",
    "# path_save = f'{savefolder}/{os.path.basename(train_dir)}'\n",
    "\n",
    "# for epoch in range(0, epochs):\n",
    "#     for A, B in loader:\n",
    "#         w_A = A['latents'].cuda()\n",
    "#         landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "#         images_A = A['images'].cuda()\n",
    "#         image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "#         w_B = B['latents'].cuda()\n",
    "#         landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "#         images_B = B['images'].cuda()\n",
    "#         image_masks_B = B['image_masks'].cuda()\n",
    "\n",
    "\n",
    "#         # 1) Calculate parameters from latents\n",
    "#         #\n",
    "#         # params_A = dfr(w_A.view(w_A.shape[0], -1))\n",
    "#         # params_B = dfr(w_B.view(w_B.shape[0], -1))\n",
    "#         params_A = dfr(nn.Flatten()(w_A))\n",
    "#         params_B = dfr(nn.Flatten()(w_B))\n",
    "        \n",
    "        \n",
    "#         # 2) Transfer semantic \"style\" (e.g. pose) from one latent to another.\n",
    "#         #\n",
    "#         w_Ahat = rignet(w_B, params_A, labels_in) # Transfer semantic params from A to latent B\n",
    "#         w_Bhat = rignet(w_A, params_B, labels_in) # Transfer semantic params from B to latent A\n",
    "\n",
    "        \n",
    "#         # 3) Create the cycle, where transferred paramter to latent, and latent to parameter\n",
    "#         #    should match.\n",
    "#         #\n",
    "#         params_AB = dfr(nn.Flatten()(w_Ahat))\n",
    "#         params_BA = dfr(nn.Flatten()(w_Bhat))\n",
    "\n",
    "        \n",
    "#         # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "# #         params_edit_A = [x.clone() for x in params_A]\n",
    "# #         params_edit_B = [x.clone() for x in params_B]\n",
    "#         params_edit_A = [x for x in params_A]\n",
    "#         params_edit_B = [x for x in params_B]\n",
    "#         params_edit_A[2] = params_AB[2] # Ensure pose is maintained\n",
    "#         params_edit_B[2] = params_BA[2] # Ensure pose is maintained\n",
    "\n",
    "\n",
    "#         # Losses\n",
    "#         # ------------------------------------------------\n",
    "#         total_loss = 0.0\n",
    "        \n",
    "        \n",
    "#         # Latent reconstruction loss.\n",
    "#         #\n",
    "#         scale_recon = 0.1\n",
    "#         loss_recon = 0.0\n",
    "#         w_hat = rignet(w_A, params_A, labels_in) # Reconsruct self\n",
    "#         loss_recon = (w_hat - w_A).abs().square().mean() * scale_recon\n",
    "#         w_hat = rignet(w_B, params_B, labels_in) # Reconsruct self\n",
    "#         loss_recon += (w_hat - w_B).abs().square().mean() * scale_recon\n",
    "        \n",
    "        \n",
    "#         if epoch % 100 == 0:\n",
    "#             savefolder = path_save\n",
    "#         else:\n",
    "#             savefolder = None\n",
    "            \n",
    "            \n",
    "#         loss_A = cycle_editing_loss(epoch, A, params_edit_A,\n",
    "#                                     flame, flametex, render,\n",
    "#                                     savefolder, batch_size)\n",
    "#         loss_B = cycle_editing_loss(epoch, B, params_edit_B,\n",
    "#                                     flame, flametex, render,\n",
    "#                                     savefolder, batch_size)\n",
    "# #         loss_A = cycle_editing_loss(A, params_edit_B, flame, flametex)\n",
    "# #         loss_B = cycle_editing_loss(B, params_edit_A, flame, flametex)\n",
    "#         loss_edit = loss_A['all_loss'] + loss_B['all_loss']\n",
    "# #         loss_edit = loss_A['all_loss']\n",
    "        \n",
    "    \n",
    "#         total_loss += loss_recon\n",
    "#         total_loss += loss_edit\n",
    "        \n",
    "        \n",
    "#         optim.zero_grad()\n",
    "#         total_loss.backward()\n",
    "#         optim.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "#     if epoch % 100 == 0:\n",
    "#         print(\"epoch: \", epoch, \", loss_edit: \", total_loss.item(), \", loss_recon: \", loss_recon.item())\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "epochs = int(10e3)\n",
    "\n",
    "\n",
    "# shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "# onehot_cam = F.one_hot(torch.tensor([4]), 6).unsqueeze(0)\n",
    "onehot_pose = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "\n",
    "\n",
    "# labels_in = torch.cat((onehot_pose, onehot_expr), dim=0).cuda()\n",
    "labels_in = onehot_pose\n",
    "labels_in = labels_in.expand(batch_size, -1, -1)\n",
    "labels_in = labels_in.cuda()\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%m-%Y_%H.%M.%S\")     # dd/mm/YY H:M:S\n",
    "savefolder = os.path.sep.join(['./test_results', f'{dt_string}'])\n",
    "if not os.path.exists(savefolder):\n",
    "    os.makedirs(savefolder, exist_ok=True)\n",
    "    \n",
    "    \n",
    "train_dir = f'{base_path}/train_data'\n",
    "path_save = f'{savefolder}/{os.path.basename(train_dir)}'\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    for A, B in loader:\n",
    "        w_A = A['latents'].cuda()\n",
    "        landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "        images_A = A['images'].cuda()\n",
    "        image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "        \n",
    "        w_B = B['latents'].cuda()\n",
    "        landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "        images_B = B['images'].cuda()\n",
    "        image_masks_B = B['image_masks'].cuda()\n",
    "\n",
    "\n",
    "        # 1) Calculate parameters from latents\n",
    "        #\n",
    "        # params_A = dfr(w_A.view(w_A.shape[0], -1))\n",
    "        # params_B = dfr(w_B.view(w_B.shape[0], -1))\n",
    "        params_A = dfr(nn.Flatten()(w_A))\n",
    "        params_B = dfr(nn.Flatten()(w_B))\n",
    "\n",
    "\n",
    "        scale_recon = 10.0\n",
    "        loss_recon = 0.0\n",
    "        w_Ahat = rignet(w_A, params_A, labels_in) # Reconsruct self\n",
    "        loss_recon = (w_Ahat - w_A).abs().square().mean() * scale_recon\n",
    "        w_Bhat = rignet(w_B, params_B, labels_in) # Reconsruct self\n",
    "        loss_recon += (w_Bhat - w_B).abs().square().mean() * scale_recon\n",
    "\n",
    "        \n",
    "        # 2) Transfer semantic \"style\" (e.g. pose) from one latent to another.\n",
    "        #\n",
    "        w_Ahat = rignet(w_B, params_A, labels_in) # Transfer semantic params from A to latent B\n",
    "        w_Bhat = rignet(w_A, params_B, labels_in) # Transfer semantic params from B to latent A\n",
    "#         w_Bhat = rignet(w_Ahat, params_B, labels_in) # Transfer semantic params from A to latent B\n",
    "#         w_Ahat = rignet(w_Bhat, params_A, labels_in) # Transfer semantic params from B to latent A\n",
    "\n",
    "        \n",
    "        # 3) Create the cycle, where transferred paramter to latent, and latent to parameter\n",
    "        #    should match.\n",
    "        #\n",
    "        params_AB = dfr(nn.Flatten()(w_Ahat))\n",
    "        params_BA = dfr(nn.Flatten()(w_Bhat))\n",
    "\n",
    "        \n",
    "        # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "#         params_edit_A = [x.clone() for x in params_A]\n",
    "#         params_edit_B = [x.clone() for x in params_B]\n",
    "        params_edit_A = [x for x in params_A]\n",
    "        params_edit_B = [x for x in params_B]\n",
    "        \n",
    "        \n",
    "        ### TODO:\n",
    "        ### - Update below to use labels to dictate what is swapped in\n",
    "        ###   for the editing loss.\n",
    "        ###\n",
    "        params_edit_A[2] = params_AB[2] # Ensure pose is maintained\n",
    "        params_edit_B[2] = params_BA[2] # Ensure pose is maintained\n",
    "\n",
    "\n",
    "        # Losses\n",
    "        # ------------------------------------------------\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            savefolder = path_save\n",
    "        else:\n",
    "            savefolder = None\n",
    "            \n",
    "            \n",
    "        loss_A = cycle_editing_loss(epoch, A, params_edit_A,\n",
    "                                    flame, flametex, render,\n",
    "                                    savefolder, batch_size)\n",
    "        loss_B = cycle_editing_loss(epoch, B, params_edit_B,\n",
    "                                    flame, flametex, render,\n",
    "                                    savefolder, batch_size)\n",
    "\n",
    "        \n",
    "        loss_edit = loss_A['all_loss'] + loss_B['all_loss']\n",
    "        \n",
    "    \n",
    "        total_loss += loss_recon\n",
    "        total_loss += loss_edit \n",
    "        \n",
    "        \n",
    "        optim.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "            print(\"epoch: \", epoch, \", loss_edit: \", total_loss.item(), \", loss_recon: \", loss_recon.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Use below to dictate what is swapped into the \"edited\" parameter based on labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = np.array([0, 1, 2, 3, 4, 5])\n",
    "dest = np.array([0,0,0,0,0,0])\n",
    "\n",
    "onehot_cam = F.one_hot(torch.tensor([4]), 6).unsqueeze(0)\n",
    "onehot_pose = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "labels_in = onehot_cam + onehot_pose\n",
    "# labels_in = torch.cat((onehot_pose, onehot_cam), dim=0)\n",
    "print(labels_in.shape)\n",
    "# labels_in = onehot_pose\n",
    "labels_in = labels_in.expand(batch_size, -1, -1)\n",
    "\n",
    "for label in labels_in:\n",
    "    idx = torch.where(label == 1)[-1]\n",
    "#     print(idx)\n",
    "#     print(test[idx])\n",
    "    dest[idx] = source[idx]\n",
    "    \n",
    "print(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pre-trained DFR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "\n",
    "for A, B in loader:\n",
    "    w_A = A['latents'].cuda()\n",
    "    landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "    images_A = A['images'].cuda()\n",
    "    image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "    w_B = B['latents'].cuda()\n",
    "    landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "    images_B = B['images'].cuda()\n",
    "    image_masks_B = B['image_masks'].cuda()\n",
    "#     print(A.keys())\n",
    "#     print(B.keys())\n",
    "    break\n",
    "    \n",
    "param = dfr(nn.Flatten()(w_A))\n",
    "\n",
    "# shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "shape, expression, pose, tex, cam, lights = param\n",
    "vertices, landmarks2d, landmarks3d = flame(shape_params=shape,\n",
    "                                           expression_params=expression,\n",
    "                                           pose_params=pose)\n",
    "\n",
    "# render\n",
    "#\n",
    "albedos = flametex(tex) / 255.\n",
    "\n",
    "trans_vertices = util.batch_orth_proj(vertices, cam);\n",
    "trans_vertices[..., 1:] = - trans_vertices[..., 1:]\n",
    "landmarks2d = util.batch_orth_proj(landmarks2d, cam);\n",
    "landmarks2d[..., 1:] = - landmarks2d[..., 1:]\n",
    "landmarks3d = util.batch_orth_proj(landmarks3d, cam);\n",
    "landmarks3d[..., 1:] = - landmarks3d[..., 1:]\n",
    "\n",
    "\n",
    "## render\n",
    "albedos = flametex(tex) / 255.\n",
    "ops = render(vertices, trans_vertices, albedos, lights)\n",
    "\n",
    "predicted_images = ops['images']\n",
    "# if savefolder is not None:\n",
    "#     bsize = range(0, batch_size_save)\n",
    "#     shape_images = render.render_shape(vertices, trans_vertices, images)\n",
    "#     save_rendered_imgs(\n",
    "#         savefolder,\n",
    "#         epoch,\n",
    "#         images[bsize].clone(),\n",
    "#         landmarks_2d_gt.clone(),\n",
    "#         landmarks2d.clone(),\n",
    "#         landmarks3d.clone(),\n",
    "#         predicted_images[bsize].detach().cpu().float().clone(),\n",
    "#         shape_images[bsize].clone(),\n",
    "#         albedos[bsize].clone(),\n",
    "#         ops['albedo_images'].detach().cpu().clone()[bsize],                        \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_images.shape)\n",
    "plt.imshow(predicted_images[2].permute(1,2,0).detach().cpu()); plt.show()\n",
    "plt.imshow(ops['albedo_images'][2].detach().cpu().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom (my_pytorch17_py36)",
   "language": "python",
   "name": "my_pytorch17_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
