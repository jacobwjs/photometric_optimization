{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = f'{Path.home()}/SageMaker'\n",
    "print(base_path)\n",
    "%ls $base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_float_img(x):\n",
    "#     if len(x.size()) > 3:\n",
    "#         x = x.squeeze()\n",
    "    return x.mul_(127.5/255.).add_(0.5)\n",
    "\n",
    "\n",
    "def convert_to_uint8(images):\n",
    "    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n",
    "    scale = 255 / 2\n",
    "    images = images.mul(scale) \\\n",
    "            .add_(0.5 + scale) \\\n",
    "            .clamp(0, 255) \\\n",
    "            .permute(0, 2, 3, 1) \\\n",
    "            .to('cpu', torch.uint8) \\\n",
    "            .squeeze()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StyleEncoder(nn.Module):\n",
    "#     def __init__(self, dims_in=(18,512), dims_out=(18,32)):\n",
    "#         super(StyleEncoder, self).__init__()\n",
    "        \n",
    "#         self.dims_in = dims_in\n",
    "#         self.dims_out = dims_out\n",
    "        \n",
    "#         self.layers = [nn.Linear(dims_in[-1], dims_out[-1]) \\\n",
    "#                        for _ in range(0, dims_in[0])]\n",
    "#         # Using ModuleList so that this layer list can be moved to CUDA                      \n",
    "#         self.layers = torch.nn.ModuleList(self.layers)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         ''' \n",
    "#         Pass each style dimension from w+ through the\n",
    "#         independent linear mapping network\n",
    "        \n",
    "#         args:\n",
    "#             x: w+ used with StyleGAN(2); dims=(18,512)\n",
    "#         returns:\n",
    "#             outputs: independent linear mapping to lower dimensional\n",
    "#                      space; dims=(18,32)\n",
    "#         '''\n",
    "#         outputs = []\n",
    "#         for style, layer in zip(x, self.layers):\n",
    "#             outputs.append(layer(style).unsqueeze(0))\n",
    "        \n",
    "#         outputs = torch.cat(outputs, dim=0)\n",
    "#         return outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = StyleEncoder()\n",
    "# enc = enc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn((3, 18, 512))\n",
    "# print(x.shape)\n",
    "\n",
    "# out = enc(x.cuda())\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define decoder\n",
    "- concatenates each output dimension with control parameters, and similar to the encoder, independently linearly decodes each dimension back to dimensions of w+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StyleDecoder(nn.Module):\n",
    "#     def __init__(self, dims_in=(18, 268), dims_out=(18, 512)):\n",
    "#         super(StyleDecoder, self).__init__()\n",
    "        \n",
    "#         self.dims_in = dims_in\n",
    "#         self.dims_out = dims_out\n",
    "        \n",
    "#         ### TODO:\n",
    "#         ### - Update this to produce the correct results based on dimensions\n",
    "#         ###   of input and output.\n",
    "#         ###\n",
    "#         self.layers = [nn.Linear(dims_in[-1], dims_out[-1]) \\\n",
    "#                        for _ in range(0, dims_in[0])]\n",
    "        \n",
    "#         # Using ModuleList so that this layer list can be moved to CUDA\n",
    "#         #\n",
    "#         self.layers = torch.nn.ModuleList(self.layers)\n",
    "        \n",
    "#     ### TODO\n",
    "#     ### - Finish me\n",
    "#     def forward(self, w_enc, params):\n",
    "#         ''' \n",
    "#         Concatenate x and p and then decode.\n",
    "        \n",
    "#         Args:\n",
    "#             w_enc: Encoded latent\n",
    "#             params: 3DMM params\n",
    "        \n",
    "#         Returns:\n",
    "#             Decoded output\n",
    "#         ''' \n",
    "#         # Flatten lighting parameter to match dimensions of other parameters in list. \n",
    "#         #\n",
    "#         batch_size = params[0].shape[0]\n",
    "#         params = list(params)\n",
    "#         params[-1] = params[-1].view(batch_size, -1) # [B,9,3] -> [B,27]\n",
    "        \n",
    "        \n",
    "#         # Concatenate all parameters\n",
    "#         #\n",
    "#         params_flat = torch.cat(params, dim=-1)\n",
    "#         style_dims = self.dims_in[0] # 18\n",
    "#         params_flat = params_flat.unsqueeze(1).expand(-1, style_dims, -1) \n",
    "\n",
    "        \n",
    "#         # This result is fed into the decoder.\n",
    "#         #\n",
    "#         outputs = []\n",
    "#         x = torch.cat((w_enc, params_flat), dim=-1)\n",
    "         \n",
    "    \n",
    "#         # Treat each \"style_dim\" independently, therefore pass each\n",
    "#         # through an independent linear layer.\n",
    "#         #\n",
    "#         for idx, layer in enumerate(self.layers):\n",
    "#             out = layer(x[:,idx,:])\n",
    "#             out = out.unsqueeze(1) # [B,512] -> [B,1,512]\n",
    "#             outputs.append(out)\n",
    "    \n",
    "    \n",
    "#         outputs = torch.cat(outputs, dim=1) # [B,18,512]\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate models \n",
    "- DFR regressor\n",
    "- Generator (StyleGAN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda'\n",
    "# dfr = load.dfr(base_path, device)\n",
    "# g_ema = load.generator(base_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1) Regress render parameters **p** i.e. $F(\\textbf{w}_{+}) = \\textbf{p}$ <br>\n",
    "2) Pass $\\textbf{w}_{+}$ through encoder, producing lower dimensional vector **l** <br>\n",
    "3) Concatenate **l** with **p** and pass through decoder to produce vector **d** (18,512) <br>\n",
    "4) Add $\\textbf{w}_{+}$ and **d** to produce final latent $\\hat{w}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 3\n",
    "# with torch.no_grad():\n",
    "# #     images = []\n",
    "#     batch_latents = g_ema.get_latent(torch.randn((batch_size,\n",
    "#                                                   18,\n",
    "#                                                   512)).to(device)) #.to('cpu')\n",
    "\n",
    "# #     imgs_gen, _ = g_ema([batch_latents.to(device)], **kwargs)\n",
    "# #     imgs_gen = imgs_gen.cpu()\n",
    "# #     images.append(normalize_float_img(imgs_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # shape, expression, pose, tex, cam, lights = dfr(batch_latents.view(3,-1))\n",
    "# # print(shape.shape)\n",
    "# # print(expression.shape)\n",
    "# # print(pose.shape)\n",
    "# # print(tex.shape)\n",
    "# # print(cam.shape)\n",
    "# # print(lights.shape)\n",
    "\n",
    "# p = dfr(batch_latents.view(batch_size,-1))\n",
    "\n",
    "# # for param in p:\n",
    "# #     print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = enc(batch_latents)\n",
    "# # print(l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec = StyleDecoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # p = dfr(batch_latents.view(batch_size,-1))\n",
    "# # l = enc(batch_latents)\n",
    "# out = dec(l, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in out:\n",
    "#     print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RigNet\n",
    "- Encapsulates encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RigNet(nn.Module):\n",
    "#     def __init__(self,\n",
    "#             dim_enc_in=(18,512),\n",
    "#             dim_enc_out=(18,32),\n",
    "#             dim_dec_in=(18, 268),\n",
    "#             dim_dec_out=(18, 512)):\n",
    "        \n",
    "#         super(RigNet, self).__init__()\n",
    "        \n",
    "#         self.encoder = StyleEncoder(dim_enc_in, dim_enc_out)\n",
    "#         self.decoder = StyleDecoder(dim_dec_in, dim_dec_out)\n",
    "        \n",
    "#     def forward(self, w_plus, params):\n",
    "#         '''\n",
    "#         Encode, \"mix\" w/ 3DMM params, and decode.\n",
    "        \n",
    "#         Args:\n",
    "#             w_plus: Latent from mapping network of StyleGAN(2) generator.\n",
    "#             params: 3DMM params from DFR.\n",
    "            \n",
    "#         Returns:\n",
    "#             d_plus: Latent from \"mixing\" 3DMM with latent 'w_plus'\n",
    "#         '''\n",
    "#         x = self.encoder(w_plus)\n",
    "#         x = self.decoder(x, params)\n",
    "#         d_plus = torch.add(x, w_plus)\n",
    "        \n",
    "#         return d_plus\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rignet = RigNet().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full working example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import SGD, Adam, Adadelta, Adagrad, Adamax, ASGD, LBFGS, RMSprop, Rprop\n",
    "# from tqdm.auto import tqdm, trange\n",
    "# # from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# #optimizer = Adam(model.parameters(), lr=1e-3) # default Adam\n",
    "# # optimizer = SGD(model.parameters(), lr=.1, momentum=.9) # default SGD\n",
    "# #optimizer = SGD(model.parameters(), lr=1e-3, momentum=.9) # original SGD\n",
    "# # optimizer = Adadelta(rignet.parameters(), lr=0.01) # default Adadelta\n",
    "# #optimizer = Adagrad(model.parameters()) # default Adagrad\n",
    "# #optimizer = Adamax(model.parameters()) # default Adamax\n",
    "# #optimizer = ASGD(model.parameters()) # default ASGD\n",
    "# #optimizer = LBFGS(model.parameters()) # default LBFGS\n",
    "# #optimizer = RMSprop(model.parameters()) # default RMSprop\n",
    "# #optimizer = Rprop(model.parameters()) # default Rprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.RigNet import RigNet\n",
    "\n",
    "# rignet = RigNet().cuda()\n",
    "# dfr = load.dfr(base_path, device)\n",
    "\n",
    "# optimizer = Adadelta(rignet.parameters(), lr=0.01) # default Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample the latent code and pass through mapping network\n",
    "# # producing 18 dim style code for generator.\n",
    "# #\n",
    "# for i in range(0, 5):\n",
    "#     batch_size = 5\n",
    "#     z = torch.randn((batch_size, 18, 512)).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         w = g_ema.get_latent(z)\n",
    "\n",
    "#     # 3DMM params from latent\n",
    "#     #\n",
    "#     p = dfr(w.view(batch_size, -1))\n",
    "\n",
    "#     # Generate final latent code for generator \"mixed\" w/ 3DMM params.\n",
    "#     #\n",
    "#     w_hat = rignet(w, p)\n",
    "#     print(w.shape)\n",
    "# #     print(p.shape)\n",
    "#     print(w_hat.shape)\n",
    "\n",
    "#     # L2-loss\n",
    "#     #\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = (w_hat - w).abs().square().mean()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the various RigNet losses\n",
    "**Reconstruction loss**\n",
    "- Ensure can faithfully reproduce latent codes in the training corpus\n",
    "- Want to ensure $RigNet(w,F(w)) = w$\n",
    "- Use the following $l_{2}-loss$ <br>\n",
    "$L_{rec} = ||RigNet(w, F(w)) - w ||_{2}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training corpus for reconstruction loss\n",
    "#\n",
    "batch_size = 1000\n",
    "z = torch.randn((batch_size, 18, 512)).to(device)\n",
    "# with torch.no_grad():\n",
    "#     w_plus = g_ema.get_latent(z)\n",
    "    \n",
    "w_plus = g_ema.get_latent(z).detach()\n",
    "\n",
    "print(w_plus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rignet = RigNet().cuda()\n",
    "dfr = load.dfr(base_path, device)\n",
    "\n",
    "optimizer = Adam(rignet.parameters(), lr=1e-3) # default Adam # Trains\n",
    "# optimizer = Adadelta(rignet.parameters(), lr=0.001) # default Adadelta # Does not train??\n",
    "# optimizer = SGD(rignet.parameters(), lr=.1, momentum=.9) # default SGD # Trains\n",
    "# optimizer = RMSprop(rignet.parameters()) # default RMSprop # Trains\n",
    "# optimizer = ASGD(rignet.parameters()) # default ASGD # Does not train??\n",
    "# optimizer = Adamax(rignet.parameters()) # default Adamax # Trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader to batch up latents in consumable chunks.\n",
    "#\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "loader = DataLoader(w_plus, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "# pbar = tqdm(range(0, epochs), dynamic_ncols=True, smoothing=0.01)\n",
    "pbar = trange(epochs, dynamic_ncols=True, smoothing=0.01)\n",
    "for epoch in pbar:\n",
    "#     print(\"epoch: \", epoch)\n",
    "    for step, w in enumerate(loader):\n",
    "#         print(w.shape)\n",
    "        p = dfr(w.view(batch_size, -1))\n",
    "#         print(p.shape)\n",
    "        w_hat = rignet(w, p)\n",
    "\n",
    "        # L2-loss\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        loss = (w_hat - w).abs().square().mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description\n",
    "        (\n",
    "            (\n",
    "                f\"total: {loss:.4f}; recon: {loss:.4f}; \"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cycle-consistent editing loss\n",
    "1. Create small dataset in memory to test\n",
    "    - latents **w** and **v**\n",
    "    - corresponding pose parameters **$p_{w}$** and **$p_{v}$**\n",
    "    - corresponding images **$I_{w}$** and **$I_{v}$**\n",
    "    - landmarks, masks, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam, Adadelta, Adagrad, Adamax, ASGD, LBFGS, RMSprop, Rprop\n",
    "from tqdm.auto import tqdm, trange\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#optimizer = Adam(model.parameters(), lr=1e-3) # default Adam\n",
    "# optimizer = SGD(model.parameters(), lr=.1, momentum=.9) # default SGD\n",
    "#optimizer = SGD(model.parameters(), lr=1e-3, momentum=.9) # original SGD\n",
    "# optimizer = Adadelta(rignet.parameters(), lr=0.01) # default Adadelta\n",
    "#optimizer = Adagrad(model.parameters()) # default Adagrad\n",
    "#optimizer = Adamax(model.parameters()) # default Adamax\n",
    "#optimizer = ASGD(model.parameters()) # default ASGD\n",
    "#optimizer = LBFGS(model.parameters()) # default LBFGS\n",
    "#optimizer = RMSprop(model.parameters()) # default RMSprop\n",
    "#optimizer = Rprop(model.parameters()) # default Rprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# rignet = load.rignet(base_path).cuda()\n",
    "dfr = load.dfr(base_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rignet = load.rignet(base_path, one_hot=True).cuda()\n",
    "rignet = load.rignet(base_path, one_hot=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sys.path.append(f'{base_path}/stylegan2_pytorch')\n",
    "g_ema = load.generator(base_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "\n",
    "with open(f'{base_path}/photometric_optimization/configs/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "    \n",
    "model_params = config['model_params']\n",
    "path_pretrained = f'{base_path}/pretrained_models'\n",
    "model_params['flame_model_path'] = f'{path_pretrained}/generic_model.pkl'\n",
    "model_params['flame_lmk_embedding_path'] = f'{path_pretrained}/landmark_embedding.npy'\n",
    "model_params['tex_space_path'] = f'{path_pretrained}/FLAME_texture.npz'\n",
    "\n",
    "\n",
    "# del render\n",
    "render = load.renderer(base_path, model_params).cuda()\n",
    "flame = load.flame(base_path, model_params).cuda()\n",
    "flametex = load.flametex(base_path, model_params).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Produce latents\n",
    "# #\n",
    "# batch_size = 4\n",
    "# total_size = batch_size * 1  \n",
    "\n",
    "# w = g_ema.get_latent(torch.randn((total_size, 18, 512)).to(device)).detach()\n",
    "# v = g_ema.get_latent(torch.randn((total_size, 18, 512)).to(device)).detach()\n",
    "\n",
    "# # Produce parameters\n",
    "# #\n",
    "# ### TODO:\n",
    "# ### DFR model should not require inputs to be reshaped. Should be\n",
    "# ### logic inside the model itself.\n",
    "# ###\n",
    "# p_w = dfr(w.view(w.shape[0], -1))\n",
    "# p_v = dfr(v.view(v.shape[0], -1))\n",
    "\n",
    "# # Generate images\n",
    "# #\n",
    "# w_mean = g_ema.mean_latent(4096) # For truncation.\n",
    "# noise = g_ema.make_noise()\n",
    "\n",
    "\n",
    "# kwargs = {\n",
    "#     'truncation_latent': w_mean,\n",
    "#     'truncation': 0.7,\n",
    "#     'noise': noise,\n",
    "#     'randomize_noise': False,\n",
    "#     'input_is_latent': True,\n",
    "#     'return_latents': False\n",
    "# }\n",
    "\n",
    "\n",
    "# # del I_w\n",
    "# # del I_v\n",
    "# I_w, _ = g_ema([w.to(device)], **kwargs)\n",
    "# I_w = I_w.cpu()\n",
    "# I_v, _ = g_ema([v.to(device)], **kwargs)\n",
    "# I_v = I_v.cpu()\n",
    "\n",
    "\n",
    "# # Normalize and interpolate to size expected by flame and landmark models.\n",
    "# #\n",
    "\n",
    "# dims = (224, 224)\n",
    "# I_w = normalize_float_img(I_w)\n",
    "# # I_w = torch.cat(I_w, dim=0)\n",
    "# I_w = F.interpolate(I_w, dims)\n",
    "\n",
    "# I_v = normalize_float_img(I_v)\n",
    "# # I_v = torch.cat(I_v, dim=0)\n",
    "# I_v = F.interpolate(I_v, dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(convert_to_uint8(I_w[0].unsqueeze(0))); plt.axis('off'); plt.show(); \n",
    "# plt.imshow(convert_to_uint8(I_v[0].unsqueeze(0))); plt.axis('off'); plt.show(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, reuse DFR dataset so\n",
    "# don't need to calculate everything.\n",
    "#\n",
    "\n",
    "from torch.utils import data\n",
    "import glob\n",
    "\n",
    "\n",
    "class DatasetSiamese(data.Dataset):\n",
    "    def __init__(self, path_to_dir_A, path_to_dir_B):\n",
    "        self.path_A = path_to_dir_A\n",
    "        self.path_B = path_to_dir_B\n",
    "        self.files_A = glob.glob1(self.path_A, '*.pkl')\n",
    "        self.files_B = glob.glob1(self.path_B, '*.pkl')\n",
    "        \n",
    "    def __len__(self):\n",
    "        len_A = len(glob.glob1(self.path_A, '*.pkl'))\n",
    "        len_B = len(glob.glob1(self.path_A, '*.pkl'))\n",
    "        assert len_A == len_B\n",
    "        return len_A\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = str(index).zfill(6) + '.pkl'\n",
    "        example_A = torch.load(f'{self.path_A}/{filename}')\n",
    "        example_B = torch.load(f'{self.path_B}/{filename}')\n",
    "        return example_A, example_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_A = f'{base_path}/train_data/rignet_testing_A'\n",
    "print(\"train_dir: \", train_dir_A)\n",
    "train_dir_B = f'{base_path}/train_data/rignet_testing_B'\n",
    "print(\"train_dir: \", train_dir_B)\n",
    "dataset = DatasetSiamese(train_dir_A, train_dir_B)\n",
    "\n",
    "batch_size = 4\n",
    "sampler = None\n",
    "# # if args.distributed:\n",
    "# #     sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "# #         dataset,\n",
    "# #         num_replicas=torch.cuda.device_count(),\n",
    "# # # #             num_replicas=4,\n",
    "# # #             rank = args.local_rank,\n",
    "# #     )\n",
    "\n",
    "loader = data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=(sampler is None),\n",
    "    num_workers=0,\n",
    "#     collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    sampler=sampler,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "### TODO:\n",
    "### - Does not work. Looks for size() in Dataset as if it's a tensor.\n",
    "###   How to update to properly work?\n",
    "###\n",
    "# dataset = torch.utils.data.TensorDataset(dataset_A, dataset_B)\n",
    "# dataloader = data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "# # for index, (xb1, xb2) in enumerate(dataloader):\n",
    "\n",
    "\n",
    "# final_dataset = torch.utils.data.ConcatDataset((dataset_A, dataset_B))\n",
    "# loader = data.DataLoader(\n",
    "#     dataset=final_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=(sampler is None),\n",
    "#     num_workers=0,\n",
    "#     pin_memory=True,\n",
    "#     sampler=sampler,\n",
    "#     drop_last=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for A, B in loader:\n",
    "    w_A = A['latents'].cuda()\n",
    "    landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "    images_A = A['images'].cuda()\n",
    "    image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "    w_B = B['latents'].cuda()\n",
    "    landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "    images_B = B['images'].cuda()\n",
    "    image_masks_B = B['image_masks'].cuda()\n",
    "#     print(A.keys())\n",
    "#     print(B.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot conditional labels for supporting RigNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_pose = F.one_hot(torch.tensor([4]), 6).unsqueeze(0)\n",
    "onehot_expr = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "\n",
    "# labels_in = F.one_hot(torch.tensor([4]), 6).unsqueeze(0).expand(4,-1,-1)\n",
    "# labels_in = labels_in.cuda()\n",
    "\n",
    "# labels_in = torch.cat((onehot_pose, onehot_expr), dim=0).cuda()\n",
    "\n",
    "labels_in = onehot_pose\n",
    "\n",
    "labels_in = labels_in.expand(batch_size, -1, -1)\n",
    "labels_in = labels_in.cuda()\n",
    "\n",
    "print(labels_in.shape)\n",
    "print(onehot_pose.shape)\n",
    "print(\"labels_in: \", labels_in)\n",
    "print(\"labels_pose: \", onehot_pose)\n",
    "print(\"labels_expr: \", onehot_expr)\n",
    "print(onehot_pose + onehot_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Calculate parameters from latents\n",
    "#\n",
    "# params_A = dfr(w_A.view(w_A.shape[0], -1))\n",
    "# params_B = dfr(w_B.view(w_B.shape[0], -1))\n",
    "\n",
    "params_A = dfr(nn.Flatten()(w_A))\n",
    "params_B = dfr(nn.Flatten()(w_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Transfer semantic \"style\" (e.g. pose) from one latent to another.\n",
    "#\n",
    "w_Ahat = rignet(w_B, params_A, labels_in) # Transfer semantic params from A to latent B\n",
    "w_Bhat = rignet(w_A, params_B, labels_in) # Transfer semantic params from B to latent A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create the cycle, where transferred paramter to latent, and latent to parameter\n",
    "#    should match.\n",
    "#\n",
    "params_AB = dfr(nn.Flatten()(w_Ahat))\n",
    "params_BA = dfr(nn.Flatten()(w_Bhat))\n",
    "\n",
    "# shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "params_edit_A = [x.clone() for x in params_A]\n",
    "params_edit_A[2] = params_AB[2] # Ensure pose is maintained\n",
    "\n",
    "params_edit_B = [x.clone() for x in params_B]\n",
    "params_edit_B[2] = params_BA[2] # Ensure pose is maintained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the rendering and return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_editing_loss(epoch, example, param, _flame, _flametex, _render,\n",
    "                       savefolder=None, batch_size_save=4):\n",
    "    import util\n",
    "    \n",
    "    loss_mse = nn.MSELoss().cuda()\n",
    "    \n",
    "#     w_A = A['latents'].cuda()\n",
    "#     landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "#     images_A = A['images'].cuda()\n",
    "#     image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "#     w_B = B['latents'].cuda()\n",
    "#     landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "#     images_B = B['images'].cuda()\n",
    "#     image_masks_B = B['image_masks'].cuda()\n",
    "\n",
    "    latents = example['latents'].cuda()\n",
    "    landmarks_2d_gt = example['landmarks_2d_gt'].cuda()\n",
    "    landmarks_3d_gt = example['landmarks_3d_gt'].cuda()\n",
    "    images = example['images'].cuda()\n",
    "    image_masks = example['image_masks'].cuda()\n",
    "    \n",
    "    \n",
    "    # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "    shape, expression, pose, tex, cam, lights = param\n",
    "    vertices, landmarks2d, landmarks3d = _flame(shape_params=shape,\n",
    "                                               expression_params=expression,\n",
    "                                               pose_params=pose)\n",
    "\n",
    "    # render\n",
    "    #\n",
    "    albedos = _flametex(tex) / 255.\n",
    "    \n",
    "    trans_vertices = util.batch_orth_proj(vertices, cam);\n",
    "    trans_vertices[..., 1:] = - trans_vertices[..., 1:]\n",
    "    landmarks2d = util.batch_orth_proj(landmarks2d, cam);\n",
    "    landmarks2d[..., 1:] = - landmarks2d[..., 1:]\n",
    "    landmarks3d = util.batch_orth_proj(landmarks3d, cam);\n",
    "    landmarks3d[..., 1:] = - landmarks3d[..., 1:]\n",
    "    \n",
    "    losses = {}\n",
    "    losses['landmark_2d'] = util.l2_distance(landmarks2d[:, :, :2],\n",
    "                                              landmarks_2d_gt[:, :, :2]) * 10.0\n",
    "    losses['landmark_3d'] = util.l2_distance(landmarks3d[:, :, :2],\n",
    "                                              landmarks_3d_gt[:, :, :2]) * 10.0\n",
    "    losses['shape_reg'] = (torch.sum(shape ** 2) / 2) * 1e-4\n",
    "    losses['expression_reg'] = (torch.sum(expression ** 2) / 2) * 1e-4\n",
    "    losses['pose_reg'] = (torch.sum(pose ** 2) / 2) * 1e-4 #config.w_pose_reg\n",
    "\n",
    "#     # Regularize learned texture.\n",
    "#     #\n",
    "#     losses['texture_reg'] = loss_mse(albedos, texture_mean.repeat(args.batch_size, 1, 1, 1)) \n",
    "    \n",
    "    \n",
    "    ## render\n",
    "    albedos = _flametex(tex) / 255.\n",
    "    ops = _render(vertices, trans_vertices, albedos, lights)\n",
    "\n",
    "    predicted_images = ops['images']\n",
    "    losses['photometric_texture'] = (image_masks * (predicted_images - images).abs()).mean() \\\n",
    "                                    * 1.0 #config.w_pho\n",
    "\n",
    "\n",
    "    all_loss = 0.\n",
    "    for key in losses.keys():\n",
    "        all_loss = all_loss + losses[key]\n",
    "#         losses_to_plot[key].append(losses[key].item()) # Store for plotting later.\n",
    "\n",
    "\n",
    "    losses['all_loss'] = all_loss\n",
    "#     losses_to_plot['all_loss'].append(losses['all_loss'].item())\n",
    "\n",
    "\n",
    "    if savefolder is not None:\n",
    "        bsize = range(0, batch_size_save)\n",
    "        shape_images = render.render_shape(vertices, trans_vertices, images)\n",
    "        save_rendered_imgs(\n",
    "            savefolder,\n",
    "            epoch,\n",
    "            images[bsize].clone(),\n",
    "            landmarks_2d_gt.clone(),\n",
    "            landmarks2d.clone(),\n",
    "            landmarks3d.clone(),\n",
    "            predicted_images[bsize].detach().cpu().float().clone(),\n",
    "            shape_images[bsize].clone(),\n",
    "            albedos[bsize].clone(),\n",
    "            ops['albedo_images'].detach().cpu().clone()[bsize],                        \n",
    "        )\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_A = cycle_editing_loss(A, params_edit_A, flame, flametex, render)\n",
    "loss_B = cycle_editing_loss(B, params_edit_B, flame, flametex, render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(\n",
    "    rignet.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.0001 # config.e_wd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = loss_A['all_loss'] + loss_B['all_loss']\n",
    "\n",
    "optim.zero_grad()\n",
    "total_loss.backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "del dfr\n",
    "dfr = load.dfr(base_path, load_weights=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(f'{base_path}/stylegan2_pytorch')\n",
    "g_ema = load.generator(base_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(f'{base_path}/photometric_optimization/configs/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "    \n",
    "model_params = config['model_params']\n",
    "path_pretrained = f'{base_path}/pretrained_models'\n",
    "model_params['flame_model_path'] = f'{path_pretrained}/generic_model.pkl'\n",
    "model_params['flame_lmk_embedding_path'] = f'{path_pretrained}/landmark_embedding.npy'\n",
    "model_params['tex_space_path'] = f'{path_pretrained}/FLAME_texture.npz'\n",
    "\n",
    "\n",
    "# del render\n",
    "render = load.renderer(base_path, model_params).cuda()\n",
    "flame = load.flame(base_path, model_params).cuda()\n",
    "flametex = load.flametex(base_path, model_params).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rendered_imgs(savefolder, \n",
    "                       epoch,\n",
    "                       images,\n",
    "                       landmarks_gt,\n",
    "                       landmarks2d,\n",
    "                       landmarks3d=None,\n",
    "                       predicted_images=None,\n",
    "                       shape_images=None,\n",
    "                       albedos=None,\n",
    "                       albedo_images=None):\n",
    "\n",
    "    import torchvision\n",
    "    import util\n",
    "    import cv2\n",
    "    \n",
    "    if not os.path.exists(savefolder):\n",
    "        os.makedirs(savefolder, exist_ok=True)\n",
    "    \n",
    "    grids = {}\n",
    "    grids['images'] = torchvision.utils.make_grid(images).detach().cpu()\n",
    "    grids['landmarks_gt'] = torchvision.utils.make_grid(\n",
    "        util.tensor_vis_landmarks(images.clone().detach(), landmarks_gt))\n",
    "    grids['landmarks2d'] = torchvision.utils.make_grid(\n",
    "        util.tensor_vis_landmarks(images, landmarks2d))\n",
    "    \n",
    "    if landmarks3d is not None:\n",
    "        grids['landmarks3d'] = torchvision.utils.make_grid(\n",
    "            util.tensor_vis_landmarks(images, landmarks3d))\n",
    "    if albedo_images is not None:\n",
    "        grids['albedoimage'] = torchvision.utils.make_grid(albedo_images)\n",
    "    if predicted_images is not None:\n",
    "        grids['render'] = torchvision.utils.make_grid(predicted_images)\n",
    "    if shape_images is not None:\n",
    "        grids['shape'] = torchvision.utils.make_grid(\n",
    "            F.interpolate(shape_images, [224, 224])).detach().float().cpu()\n",
    "    if albedos is not None:\n",
    "        grids['tex'] = torchvision.utils.make_grid(F.interpolate(albedos, [224, 224])).detach().cpu()\n",
    "\n",
    "    grid = torch.cat(list(grids.values()), 1)\n",
    "    grid_image = (grid.numpy().transpose(1, 2, 0).copy() * 255)[:, :, [2, 1, 0]]\n",
    "    grid_image = np.minimum(np.maximum(grid_image, 0), 255).astype(np.uint8)\n",
    "\n",
    "    cv2.imwrite('{}/{}.jpg'.format(savefolder, str(epoch).zfill(6)), grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rignet\n",
    "# one_hot = True\n",
    "one_hot = True\n",
    "rignet = load.rignet(base_path, one_hot=one_hot).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam, Adadelta, Adagrad, Adamax, ASGD, LBFGS, RMSprop, Rprop\n",
    "\n",
    "\n",
    "# optim = torch.optim.Adam(\n",
    "#     rignet.parameters(),\n",
    "#     lr=1e-3,    \n",
    "# #     weight_decay=0.00001 # config.e_wd\n",
    "# )\n",
    "\n",
    "# optim = Adamax(rignet.parameters()) # default Adamax # Trains\n",
    "optim = Adadelta(rignet.parameters(), lr=0.01) # default Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# import os\n",
    "\n",
    "\n",
    "# epochs = int(10e3)\n",
    "\n",
    "\n",
    "# # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "# # onehot_cam = F.one_hot(torch.tensor([4]), 6).unsqueeze(0)\n",
    "# onehot_pose = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "# # labels_in = torch.cat((onehot_pose, onehot_expr), dim=0).cuda()\n",
    "# labels_in = onehot_pose\n",
    "# labels_in = labels_in.expand(batch_size, -1, -1)\n",
    "# labels_in = labels_in.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# now = datetime.now()\n",
    "# dt_string = now.strftime(\"%d-%m-%Y_%H.%M.%S\")     # dd/mm/YY H:M:S\n",
    "# savefolder = os.path.sep.join(['./test_results', f'{dt_string}'])\n",
    "# if not os.path.exists(savefolder):\n",
    "#     os.makedirs(savefolder, exist_ok=True)\n",
    "    \n",
    "# train_dir = f'{base_path}/train_data'\n",
    "# path_save = f'{savefolder}/{os.path.basename(train_dir)}'\n",
    "\n",
    "# for epoch in range(0, epochs):\n",
    "#     for A, B in loader:\n",
    "#         w_A = A['latents'].cuda()\n",
    "#         landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "#         images_A = A['images'].cuda()\n",
    "#         image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "#         w_B = B['latents'].cuda()\n",
    "#         landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "#         images_B = B['images'].cuda()\n",
    "#         image_masks_B = B['image_masks'].cuda()\n",
    "\n",
    "\n",
    "#         # 1) Calculate parameters from latents\n",
    "#         #\n",
    "#         # params_A = dfr(w_A.view(w_A.shape[0], -1))\n",
    "#         # params_B = dfr(w_B.view(w_B.shape[0], -1))\n",
    "#         params_A = dfr(nn.Flatten()(w_A))\n",
    "#         params_B = dfr(nn.Flatten()(w_B))\n",
    "        \n",
    "        \n",
    "#         # 2) Transfer semantic \"style\" (e.g. pose) from one latent to another.\n",
    "#         #\n",
    "#         w_Ahat = rignet(w_B, params_A, labels_in) # Transfer semantic params from A to latent B\n",
    "#         w_Bhat = rignet(w_A, params_B, labels_in) # Transfer semantic params from B to latent A\n",
    "\n",
    "        \n",
    "#         # 3) Create the cycle, where transferred paramter to latent, and latent to parameter\n",
    "#         #    should match.\n",
    "#         #\n",
    "#         params_AB = dfr(nn.Flatten()(w_Ahat))\n",
    "#         params_BA = dfr(nn.Flatten()(w_Bhat))\n",
    "\n",
    "        \n",
    "#         # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "# #         params_edit_A = [x.clone() for x in params_A]\n",
    "# #         params_edit_B = [x.clone() for x in params_B]\n",
    "#         params_edit_A = [x for x in params_A]\n",
    "#         params_edit_B = [x for x in params_B]\n",
    "#         params_edit_A[2] = params_AB[2] # Ensure pose is maintained\n",
    "#         params_edit_B[2] = params_BA[2] # Ensure pose is maintained\n",
    "\n",
    "\n",
    "#         # Losses\n",
    "#         # ------------------------------------------------\n",
    "#         total_loss = 0.0\n",
    "        \n",
    "        \n",
    "#         # Latent reconstruction loss.\n",
    "#         #\n",
    "#         scale_recon = 0.1\n",
    "#         loss_recon = 0.0\n",
    "#         w_hat = rignet(w_A, params_A, labels_in) # Reconsruct self\n",
    "#         loss_recon = (w_hat - w_A).abs().square().mean() * scale_recon\n",
    "#         w_hat = rignet(w_B, params_B, labels_in) # Reconsruct self\n",
    "#         loss_recon += (w_hat - w_B).abs().square().mean() * scale_recon\n",
    "        \n",
    "        \n",
    "#         if epoch % 100 == 0:\n",
    "#             savefolder = path_save\n",
    "#         else:\n",
    "#             savefolder = None\n",
    "            \n",
    "            \n",
    "#         loss_A = cycle_editing_loss(epoch, A, params_edit_A,\n",
    "#                                     flame, flametex, render,\n",
    "#                                     savefolder, batch_size)\n",
    "#         loss_B = cycle_editing_loss(epoch, B, params_edit_B,\n",
    "#                                     flame, flametex, render,\n",
    "#                                     savefolder, batch_size)\n",
    "# #         loss_A = cycle_editing_loss(A, params_edit_B, flame, flametex)\n",
    "# #         loss_B = cycle_editing_loss(B, params_edit_A, flame, flametex)\n",
    "#         loss_edit = loss_A['all_loss'] + loss_B['all_loss']\n",
    "# #         loss_edit = loss_A['all_loss']\n",
    "        \n",
    "    \n",
    "#         total_loss += loss_recon\n",
    "#         total_loss += loss_edit\n",
    "        \n",
    "        \n",
    "#         optim.zero_grad()\n",
    "#         total_loss.backward()\n",
    "#         optim.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "#     if epoch % 100 == 0:\n",
    "#         print(\"epoch: \", epoch, \", loss_edit: \", total_loss.item(), \", loss_recon: \", loss_recon.item())\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "epochs = int(10e3)\n",
    "\n",
    "\n",
    "# shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "# onehot_cam = F.one_hot(torch.tensor([4]), 6).unsqueeze(0)\n",
    "onehot_pose = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "\n",
    "\n",
    "# labels_in = torch.cat((onehot_pose, onehot_expr), dim=0).cuda()\n",
    "labels_in = onehot_pose\n",
    "labels_in = labels_in.expand(batch_size, -1, -1)\n",
    "labels_in = labels_in.cuda()\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%m-%Y_%H.%M.%S\")     # dd/mm/YY H:M:S\n",
    "savefolder = os.path.sep.join(['./test_results', f'{dt_string}'])\n",
    "if not os.path.exists(savefolder):\n",
    "    os.makedirs(savefolder, exist_ok=True)\n",
    "    \n",
    "    \n",
    "train_dir = f'{base_path}/train_data'\n",
    "path_save = f'{savefolder}/{os.path.basename(train_dir)}'\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    for A, B in loader:\n",
    "        w_A = A['latents'].cuda()\n",
    "        landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "        images_A = A['images'].cuda()\n",
    "        image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "        \n",
    "        w_B = B['latents'].cuda()\n",
    "        landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "        images_B = B['images'].cuda()\n",
    "        image_masks_B = B['image_masks'].cuda()\n",
    "\n",
    "\n",
    "        # 1) Calculate parameters from latents\n",
    "        #\n",
    "        # params_A = dfr(w_A.view(w_A.shape[0], -1))\n",
    "        # params_B = dfr(w_B.view(w_B.shape[0], -1))\n",
    "        params_A = dfr(nn.Flatten()(w_A))\n",
    "        params_B = dfr(nn.Flatten()(w_B))\n",
    "\n",
    "\n",
    "        scale_recon = 10.0\n",
    "        loss_recon = 0.0\n",
    "        w_Ahat = rignet(w_A, params_A, labels_in) # Reconsruct self\n",
    "        loss_recon = (w_Ahat - w_A).abs().square().mean() * scale_recon\n",
    "        w_Bhat = rignet(w_B, params_B, labels_in) # Reconsruct self\n",
    "        loss_recon += (w_Bhat - w_B).abs().square().mean() * scale_recon\n",
    "\n",
    "        \n",
    "        # 2) Transfer semantic \"style\" (e.g. pose) from one latent to another.\n",
    "        #\n",
    "        w_Ahat = rignet(w_B, params_A, labels_in) # Transfer semantic params from A to latent B\n",
    "        w_Bhat = rignet(w_A, params_B, labels_in) # Transfer semantic params from B to latent A\n",
    "#         w_Bhat = rignet(w_Ahat, params_B, labels_in) # Transfer semantic params from A to latent B\n",
    "#         w_Ahat = rignet(w_Bhat, params_A, labels_in) # Transfer semantic params from B to latent A\n",
    "\n",
    "        \n",
    "        # 3) Create the cycle, where transferred paramter to latent, and latent to parameter\n",
    "        #    should match.\n",
    "        #\n",
    "        params_AB = dfr(nn.Flatten()(w_Ahat))\n",
    "        params_BA = dfr(nn.Flatten()(w_Bhat))\n",
    "\n",
    "        \n",
    "        # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "#         params_edit_A = [x.clone() for x in params_A]\n",
    "#         params_edit_B = [x.clone() for x in params_B]\n",
    "        params_edit_A = [x for x in params_A]\n",
    "        params_edit_B = [x for x in params_B]\n",
    "        \n",
    "        \n",
    "        ### TODO:\n",
    "        ### - Update below to use labels to dictate what is swapped in\n",
    "        ###   for the editing loss.\n",
    "        ###\n",
    "        params_edit_A[2] = params_AB[2] # Ensure pose is maintained\n",
    "        params_edit_B[2] = params_BA[2] # Ensure pose is maintained\n",
    "\n",
    "\n",
    "        # Losses\n",
    "        # ------------------------------------------------\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            savefolder = path_save\n",
    "        else:\n",
    "            savefolder = None\n",
    "            \n",
    "            \n",
    "        loss_A = cycle_editing_loss(epoch, A, params_edit_A,\n",
    "                                    flame, flametex, render,\n",
    "                                    savefolder, batch_size)\n",
    "        loss_B = cycle_editing_loss(epoch, B, params_edit_B,\n",
    "                                    flame, flametex, render,\n",
    "                                    savefolder, batch_size)\n",
    "\n",
    "        \n",
    "        loss_edit = loss_A['all_loss'] + loss_B['all_loss']\n",
    "        \n",
    "    \n",
    "        total_loss += loss_recon\n",
    "        total_loss += loss_edit \n",
    "        \n",
    "        \n",
    "        optim.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "            print(\"epoch: \", epoch, \", loss_edit: \", total_loss.item(), \", loss_recon: \", loss_recon.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Use below to dictate what is swapped into the \"edited\" parameter based on labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = np.array([0, 1, 2, 3, 4, 5])\n",
    "dest = np.array([0,0,0,0,0,0])\n",
    "\n",
    "onehot_cam = F.one_hot(torch.tensor([4]), 6).unsqueeze(0)\n",
    "onehot_pose = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "labels_in = onehot_cam + onehot_pose\n",
    "# labels_in = torch.cat((onehot_pose, onehot_cam), dim=0)\n",
    "print(labels_in.shape)\n",
    "# labels_in = onehot_pose\n",
    "labels_in = labels_in.expand(batch_size, -1, -1)\n",
    "\n",
    "for label in labels_in:\n",
    "    idx = torch.where(label == 1)[-1]\n",
    "#     print(idx)\n",
    "#     print(test[idx])\n",
    "    dest[idx] = source[idx]\n",
    "    \n",
    "print(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pre-trained DFR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "\n",
    "for A, B in loader:\n",
    "    w_A = A['latents'].cuda()\n",
    "    landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "    images_A = A['images'].cuda()\n",
    "    image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "    w_B = B['latents'].cuda()\n",
    "    landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "    images_B = B['images'].cuda()\n",
    "    image_masks_B = B['image_masks'].cuda()\n",
    "#     print(A.keys())\n",
    "#     print(B.keys())\n",
    "    break\n",
    "    \n",
    "param = dfr(nn.Flatten()(w_A))\n",
    "\n",
    "# shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "shape, expression, pose, tex, cam, lights = param\n",
    "vertices, landmarks2d, landmarks3d = flame(shape_params=shape,\n",
    "                                           expression_params=expression,\n",
    "                                           pose_params=pose)\n",
    "\n",
    "# render\n",
    "#\n",
    "albedos = flametex(tex) / 255.\n",
    "\n",
    "trans_vertices = util.batch_orth_proj(vertices, cam);\n",
    "trans_vertices[..., 1:] = - trans_vertices[..., 1:]\n",
    "landmarks2d = util.batch_orth_proj(landmarks2d, cam);\n",
    "landmarks2d[..., 1:] = - landmarks2d[..., 1:]\n",
    "landmarks3d = util.batch_orth_proj(landmarks3d, cam);\n",
    "landmarks3d[..., 1:] = - landmarks3d[..., 1:]\n",
    "\n",
    "\n",
    "## render\n",
    "albedos = flametex(tex) / 255.\n",
    "ops = render(vertices, trans_vertices, albedos, lights)\n",
    "\n",
    "predicted_images = ops['images']\n",
    "# if savefolder is not None:\n",
    "#     bsize = range(0, batch_size_save)\n",
    "#     shape_images = render.render_shape(vertices, trans_vertices, images)\n",
    "#     save_rendered_imgs(\n",
    "#         savefolder,\n",
    "#         epoch,\n",
    "#         images[bsize].clone(),\n",
    "#         landmarks_2d_gt.clone(),\n",
    "#         landmarks2d.clone(),\n",
    "#         landmarks3d.clone(),\n",
    "#         predicted_images[bsize].detach().cpu().float().clone(),\n",
    "#         shape_images[bsize].clone(),\n",
    "#         albedos[bsize].clone(),\n",
    "#         ops['albedo_images'].detach().cpu().clone()[bsize],                        \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_images.shape)\n",
    "plt.imshow(predicted_images[2].permute(1,2,0).detach().cpu()); plt.show()\n",
    "plt.imshow(ops['albedo_images'][2].detach().cpu().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom (my_pytorch17_py36)",
   "language": "python",
   "name": "my_pytorch17_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
