{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n",
      "\u001b[0m\u001b[01;34m3DDFA_V2\u001b[0m/          \u001b[01;34mlost+found\u001b[0m/                setup.ipynb\n",
      "autostop.py        mean_landmarks.ipynb       \u001b[01;34mstylegan2_pytorch\u001b[0m/\n",
      "autostop.py.1      model.py                   \u001b[01;34mtrain_data\u001b[0m/\n",
      "\u001b[01;32mcron-shutdown.sh\u001b[0m*  \u001b[01;32mon-start.sh\u001b[0m*               train.py\n",
      "\u001b[01;34mcustom-miniconda\u001b[0m/  \u001b[01;34mphotometric_optimization\u001b[0m/  train_v2.py\n",
      "\u001b[01;34mdata\u001b[0m/              \u001b[01;34mpretrained_models\u001b[0m/         utils.ipynb\n",
      "\u001b[01;34mFace_Seg\u001b[0m/          \u001b[01;34m__pycache__\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "import load\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = f'{Path.home()}/SageMaker'\n",
    "print(base_path)\n",
    "%ls $base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['',\n",
      " '/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/my_pytorch17_py36/lib/python36.zip',\n",
      " '/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/my_pytorch17_py36/lib/python3.6',\n",
      " '/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/my_pytorch17_py36/lib/python3.6/lib-dynload',\n",
      " '/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/my_pytorch17_py36/lib/python3.6/site-packages',\n",
      " '/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/my_pytorch17_py36/lib/python3.6/site-packages/IPython/extensions',\n",
      " '/home/ec2-user/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_float_img(x):\n",
    "#     if len(x.size()) > 3:\n",
    "#         x = x.squeeze()\n",
    "    return x.mul_(127.5/255.).add_(0.5)\n",
    "\n",
    "\n",
    "def convert_to_uint8(images):\n",
    "    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n",
    "    scale = 255 / 2\n",
    "    images = images.mul(scale) \\\n",
    "            .add_(0.5 + scale) \\\n",
    "            .clamp(0, 255) \\\n",
    "            .permute(0, 2, 3, 1) \\\n",
    "            .to('cpu', torch.uint8) \\\n",
    "            .squeeze()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StyleEncoder(nn.Module):\n",
    "#     def __init__(self, dims_in=(18,512), dims_out=(18,32)):\n",
    "#         super(StyleEncoder, self).__init__()\n",
    "        \n",
    "#         self.dims_in = dims_in\n",
    "#         self.dims_out = dims_out\n",
    "        \n",
    "#         self.layers = [nn.Linear(dims_in[-1], dims_out[-1]) \\\n",
    "#                        for _ in range(0, dims_in[0])]\n",
    "#         # Using ModuleList so that this layer list can be moved to CUDA                      \n",
    "#         self.layers = torch.nn.ModuleList(self.layers)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         ''' \n",
    "#         Pass each style dimension from w+ through the\n",
    "#         independent linear mapping network\n",
    "        \n",
    "#         args:\n",
    "#             x: w+ used with StyleGAN(2); dims=(18,512)\n",
    "#         returns:\n",
    "#             outputs: independent linear mapping to lower dimensional\n",
    "#                      space; dims=(18,32)\n",
    "#         '''\n",
    "#         outputs = []\n",
    "#         for style, layer in zip(x, self.layers):\n",
    "#             outputs.append(layer(style).unsqueeze(0))\n",
    "        \n",
    "#         outputs = torch.cat(outputs, dim=0)\n",
    "#         return outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = StyleEncoder()\n",
    "# enc = enc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn((3, 18, 512))\n",
    "# print(x.shape)\n",
    "\n",
    "# out = enc(x.cuda())\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define decoder\n",
    "- concatenates each output dimension with control parameters, and similar to the encoder, independently linearly decodes each dimension back to dimensions of w+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StyleDecoder(nn.Module):\n",
    "#     def __init__(self, dims_in=(18, 268), dims_out=(18, 512)):\n",
    "#         super(StyleDecoder, self).__init__()\n",
    "        \n",
    "#         self.dims_in = dims_in\n",
    "#         self.dims_out = dims_out\n",
    "        \n",
    "#         ### TODO:\n",
    "#         ### - Update this to produce the correct results based on dimensions\n",
    "#         ###   of input and output.\n",
    "#         ###\n",
    "#         self.layers = [nn.Linear(dims_in[-1], dims_out[-1]) \\\n",
    "#                        for _ in range(0, dims_in[0])]\n",
    "        \n",
    "#         # Using ModuleList so that this layer list can be moved to CUDA\n",
    "#         #\n",
    "#         self.layers = torch.nn.ModuleList(self.layers)\n",
    "        \n",
    "#     ### TODO\n",
    "#     ### - Finish me\n",
    "#     def forward(self, w_enc, params):\n",
    "#         ''' \n",
    "#         Concatenate x and p and then decode.\n",
    "        \n",
    "#         Args:\n",
    "#             w_enc: Encoded latent\n",
    "#             params: 3DMM params\n",
    "        \n",
    "#         Returns:\n",
    "#             Decoded output\n",
    "#         ''' \n",
    "#         # Flatten lighting parameter to match dimensions of other parameters in list. \n",
    "#         #\n",
    "#         batch_size = params[0].shape[0]\n",
    "#         params = list(params)\n",
    "#         params[-1] = params[-1].view(batch_size, -1) # [B,9,3] -> [B,27]\n",
    "        \n",
    "        \n",
    "#         # Concatenate all parameters\n",
    "#         #\n",
    "#         params_flat = torch.cat(params, dim=-1)\n",
    "#         style_dims = self.dims_in[0] # 18\n",
    "#         params_flat = params_flat.unsqueeze(1).expand(-1, style_dims, -1) \n",
    "\n",
    "        \n",
    "#         # This result is fed into the decoder.\n",
    "#         #\n",
    "#         outputs = []\n",
    "#         x = torch.cat((w_enc, params_flat), dim=-1)\n",
    "         \n",
    "    \n",
    "#         # Treat each \"style_dim\" independently, therefore pass each\n",
    "#         # through an independent linear layer.\n",
    "#         #\n",
    "#         for idx, layer in enumerate(self.layers):\n",
    "#             out = layer(x[:,idx,:])\n",
    "#             out = out.unsqueeze(1) # [B,512] -> [B,1,512]\n",
    "#             outputs.append(out)\n",
    "    \n",
    "    \n",
    "#         outputs = torch.cat(outputs, dim=1) # [B,18,512]\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate models \n",
    "- DFR regressor\n",
    "- Generator (StyleGAN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda'\n",
    "# dfr = load.dfr(base_path, device)\n",
    "# g_ema = load.generator(base_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1) Regress render parameters **p** i.e. $F(\\textbf{w}_{+}) = \\textbf{p}$ <br>\n",
    "2) Pass $\\textbf{w}_{+}$ through encoder, producing lower dimensional vector **l** <br>\n",
    "3) Concatenate **l** with **p** and pass through decoder to produce vector **d** (18,512) <br>\n",
    "4) Add $\\textbf{w}_{+}$ and **d** to produce final latent $\\hat{w}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 3\n",
    "# with torch.no_grad():\n",
    "# #     images = []\n",
    "#     batch_latents = g_ema.get_latent(torch.randn((batch_size,\n",
    "#                                                   18,\n",
    "#                                                   512)).to(device)) #.to('cpu')\n",
    "\n",
    "# #     imgs_gen, _ = g_ema([batch_latents.to(device)], **kwargs)\n",
    "# #     imgs_gen = imgs_gen.cpu()\n",
    "# #     images.append(normalize_float_img(imgs_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # shape, expression, pose, tex, cam, lights = dfr(batch_latents.view(3,-1))\n",
    "# # print(shape.shape)\n",
    "# # print(expression.shape)\n",
    "# # print(pose.shape)\n",
    "# # print(tex.shape)\n",
    "# # print(cam.shape)\n",
    "# # print(lights.shape)\n",
    "\n",
    "# p = dfr(batch_latents.view(batch_size,-1))\n",
    "\n",
    "# # for param in p:\n",
    "# #     print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = enc(batch_latents)\n",
    "# # print(l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec = StyleDecoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # p = dfr(batch_latents.view(batch_size,-1))\n",
    "# # l = enc(batch_latents)\n",
    "# out = dec(l, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in out:\n",
    "#     print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RigNet\n",
    "- Encapsulates encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RigNet(nn.Module):\n",
    "#     def __init__(self,\n",
    "#             dim_enc_in=(18,512),\n",
    "#             dim_enc_out=(18,32),\n",
    "#             dim_dec_in=(18, 268),\n",
    "#             dim_dec_out=(18, 512)):\n",
    "        \n",
    "#         super(RigNet, self).__init__()\n",
    "        \n",
    "#         self.encoder = StyleEncoder(dim_enc_in, dim_enc_out)\n",
    "#         self.decoder = StyleDecoder(dim_dec_in, dim_dec_out)\n",
    "        \n",
    "#     def forward(self, w_plus, params):\n",
    "#         '''\n",
    "#         Encode, \"mix\" w/ 3DMM params, and decode.\n",
    "        \n",
    "#         Args:\n",
    "#             w_plus: Latent from mapping network of StyleGAN(2) generator.\n",
    "#             params: 3DMM params from DFR.\n",
    "            \n",
    "#         Returns:\n",
    "#             d_plus: Latent from \"mixing\" 3DMM with latent 'w_plus'\n",
    "#         '''\n",
    "#         x = self.encoder(w_plus)\n",
    "#         x = self.decoder(x, params)\n",
    "#         d_plus = torch.add(x, w_plus)\n",
    "        \n",
    "#         return d_plus\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rignet = RigNet().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full working example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import SGD, Adam, Adadelta, Adagrad, Adamax, ASGD, LBFGS, RMSprop, Rprop\n",
    "# from tqdm.auto import tqdm, trange\n",
    "# # from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# #optimizer = Adam(model.parameters(), lr=1e-3) # default Adam\n",
    "# # optimizer = SGD(model.parameters(), lr=.1, momentum=.9) # default SGD\n",
    "# #optimizer = SGD(model.parameters(), lr=1e-3, momentum=.9) # original SGD\n",
    "# # optimizer = Adadelta(rignet.parameters(), lr=0.01) # default Adadelta\n",
    "# #optimizer = Adagrad(model.parameters()) # default Adagrad\n",
    "# #optimizer = Adamax(model.parameters()) # default Adamax\n",
    "# #optimizer = ASGD(model.parameters()) # default ASGD\n",
    "# #optimizer = LBFGS(model.parameters()) # default LBFGS\n",
    "# #optimizer = RMSprop(model.parameters()) # default RMSprop\n",
    "# #optimizer = Rprop(model.parameters()) # default Rprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.RigNet import RigNet\n",
    "\n",
    "# rignet = RigNet().cuda()\n",
    "# dfr = load.dfr(base_path, device)\n",
    "\n",
    "# optimizer = Adadelta(rignet.parameters(), lr=0.01) # default Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample the latent code and pass through mapping network\n",
    "# # producing 18 dim style code for generator.\n",
    "# #\n",
    "# for i in range(0, 5):\n",
    "#     batch_size = 5\n",
    "#     z = torch.randn((batch_size, 18, 512)).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         w = g_ema.get_latent(z)\n",
    "\n",
    "#     # 3DMM params from latent\n",
    "#     #\n",
    "#     p = dfr(w.view(batch_size, -1))\n",
    "\n",
    "#     # Generate final latent code for generator \"mixed\" w/ 3DMM params.\n",
    "#     #\n",
    "#     w_hat = rignet(w, p)\n",
    "#     print(w.shape)\n",
    "# #     print(p.shape)\n",
    "#     print(w_hat.shape)\n",
    "\n",
    "#     # L2-loss\n",
    "#     #\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = (w_hat - w).abs().square().mean()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the various RigNet losses\n",
    "**Reconstruction loss**\n",
    "- Ensure can faithfully reproduce latent codes in the training corpus\n",
    "- Want to ensure $RigNet(w,F(w)) = w$\n",
    "- Use the following $l_{2}-loss$ <br>\n",
    "$L_{rec} = ||RigNet(w, F(w)) - w ||_{2}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training corpus for reconstruction loss\n",
    "#\n",
    "batch_size = 1000\n",
    "z = torch.randn((batch_size, 18, 512)).to(device)\n",
    "# with torch.no_grad():\n",
    "#     w_plus = g_ema.get_latent(z)\n",
    "    \n",
    "w_plus = g_ema.get_latent(z).detach()\n",
    "\n",
    "print(w_plus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rignet = RigNet().cuda()\n",
    "dfr = load.dfr(base_path, device)\n",
    "\n",
    "optimizer = Adam(rignet.parameters(), lr=1e-3) # default Adam # Trains\n",
    "# optimizer = Adadelta(rignet.parameters(), lr=0.001) # default Adadelta # Does not train??\n",
    "# optimizer = SGD(rignet.parameters(), lr=.1, momentum=.9) # default SGD # Trains\n",
    "# optimizer = RMSprop(rignet.parameters()) # default RMSprop # Trains\n",
    "# optimizer = ASGD(rignet.parameters()) # default ASGD # Does not train??\n",
    "# optimizer = Adamax(rignet.parameters()) # default Adamax # Trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader to batch up latents in consumable chunks.\n",
    "#\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "loader = DataLoader(w_plus, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "# pbar = tqdm(range(0, epochs), dynamic_ncols=True, smoothing=0.01)\n",
    "pbar = trange(epochs, dynamic_ncols=True, smoothing=0.01)\n",
    "for epoch in pbar:\n",
    "#     print(\"epoch: \", epoch)\n",
    "    for step, w in enumerate(loader):\n",
    "#         print(w.shape)\n",
    "        p = dfr(w.view(batch_size, -1))\n",
    "#         print(p.shape)\n",
    "        w_hat = rignet(w, p)\n",
    "\n",
    "        # L2-loss\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        loss = (w_hat - w).abs().square().mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description\n",
    "        (\n",
    "            (\n",
    "                f\"total: {loss:.4f}; recon: {loss:.4f}; \"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cycle-consistent editing loss\n",
    "1. Create small dataset in memory to test\n",
    "    - latents **w** and **v**\n",
    "    - corresponding pose parameters **$p_{w}$** and **$p_{v}$**\n",
    "    - corresponding images **$I_{w}$** and **$I_{v}$**\n",
    "    - landmarks, masks, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam, Adadelta, Adagrad, Adamax, ASGD, LBFGS, RMSprop, Rprop\n",
    "from tqdm.auto import tqdm, trange\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#optimizer = Adam(model.parameters(), lr=1e-3) # default Adam\n",
    "# optimizer = SGD(model.parameters(), lr=.1, momentum=.9) # default SGD\n",
    "#optimizer = SGD(model.parameters(), lr=1e-3, momentum=.9) # original SGD\n",
    "# optimizer = Adadelta(rignet.parameters(), lr=0.01) # default Adadelta\n",
    "#optimizer = Adagrad(model.parameters()) # default Adagrad\n",
    "#optimizer = Adamax(model.parameters()) # default Adamax\n",
    "#optimizer = ASGD(model.parameters()) # default ASGD\n",
    "#optimizer = LBFGS(model.parameters()) # default LBFGS\n",
    "#optimizer = RMSprop(model.parameters()) # default RMSprop\n",
    "#optimizer = Rprop(model.parameters()) # default Rprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output params dims:  236\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# rignet = load.rignet(base_path).cuda()\n",
    "dfr = load.dfr(base_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RigNet model...\n"
     ]
    }
   ],
   "source": [
    "# rignet = load.rignet(base_path, one_hot=True).cuda()\n",
    "rignet = load.rignet(base_path, one_hot=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to StyleGAN2 weights:  /home/ec2-user/SageMaker/pretrained_models/stylegan2-ffhq-config-f.pt\n",
      "Loading weights...  /home/ec2-user/SageMaker/pretrained_models/stylegan2-ffhq-config-f.pt\n"
     ]
    }
   ],
   "source": [
    "# sys.path.append(f'{base_path}/stylegan2_pytorch')\n",
    "g_ema = load.generator(base_path).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/my_pytorch17_py36/lib/python3.6/site-packages/pytorch3d/io/obj_io.py:457: UserWarning: Mtl file does not exist: ./data/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the FLAME Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/photometric_optimization/models/FLAME.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer('dynamic_lmk_faces_idx', torch.tensor(lmk_embeddings['dynamic_lmk_faces_idx'], dtype=torch.long))\n",
      "/home/ec2-user/SageMaker/photometric_optimization/models/FLAME.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer('dynamic_lmk_bary_coords', torch.tensor(lmk_embeddings['dynamic_lmk_bary_coords'], dtype=self.dtype))\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(f'{base_path}/photometric_optimization/config/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "    \n",
    "model_params = config['model_params']\n",
    "path_pretrained = f'{base_path}/pretrained_models'\n",
    "model_params['flame_model_path'] = f'{path_pretrained}/generic_model.pkl'\n",
    "model_params['flame_lmk_embedding_path'] = f'{path_pretrained}/landmark_embedding.npy'\n",
    "model_params['tex_space_path'] = f'{path_pretrained}/FLAME_texture.npz'\n",
    "\n",
    "\n",
    "# del render\n",
    "render = load.renderer(base_path, model_params).cuda()\n",
    "flame = load.flame(base_path, model_params).cuda()\n",
    "flametex = load.flametex(base_path, model_params).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Produce latents\n",
    "# #\n",
    "# batch_size = 4\n",
    "# total_size = batch_size * 1  \n",
    "\n",
    "# w = g_ema.get_latent(torch.randn((total_size, 18, 512)).to(device)).detach()\n",
    "# v = g_ema.get_latent(torch.randn((total_size, 18, 512)).to(device)).detach()\n",
    "\n",
    "# # Produce parameters\n",
    "# #\n",
    "# ### TODO:\n",
    "# ### DFR model should not require inputs to be reshaped. Should be\n",
    "# ### logic inside the model itself.\n",
    "# ###\n",
    "# p_w = dfr(w.view(w.shape[0], -1))\n",
    "# p_v = dfr(v.view(v.shape[0], -1))\n",
    "\n",
    "# # Generate images\n",
    "# #\n",
    "# w_mean = g_ema.mean_latent(4096) # For truncation.\n",
    "# noise = g_ema.make_noise()\n",
    "\n",
    "\n",
    "# kwargs = {\n",
    "#     'truncation_latent': w_mean,\n",
    "#     'truncation': 0.7,\n",
    "#     'noise': noise,\n",
    "#     'randomize_noise': False,\n",
    "#     'input_is_latent': True,\n",
    "#     'return_latents': False\n",
    "# }\n",
    "\n",
    "\n",
    "# # del I_w\n",
    "# # del I_v\n",
    "# I_w, _ = g_ema([w.to(device)], **kwargs)\n",
    "# I_w = I_w.cpu()\n",
    "# I_v, _ = g_ema([v.to(device)], **kwargs)\n",
    "# I_v = I_v.cpu()\n",
    "\n",
    "\n",
    "# # Normalize and interpolate to size expected by flame and landmark models.\n",
    "# #\n",
    "\n",
    "# dims = (224, 224)\n",
    "# I_w = normalize_float_img(I_w)\n",
    "# # I_w = torch.cat(I_w, dim=0)\n",
    "# I_w = F.interpolate(I_w, dims)\n",
    "\n",
    "# I_v = normalize_float_img(I_v)\n",
    "# # I_v = torch.cat(I_v, dim=0)\n",
    "# I_v = F.interpolate(I_v, dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(convert_to_uint8(I_w[0].unsqueeze(0))); plt.axis('off'); plt.show(); \n",
    "# plt.imshow(convert_to_uint8(I_v[0].unsqueeze(0))); plt.axis('off'); plt.show(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, reuse DFR dataset so\n",
    "# don't need to calculate everything.\n",
    "#\n",
    "\n",
    "from torch.utils import data\n",
    "import glob\n",
    "\n",
    "\n",
    "class DatasetSiamese(data.Dataset):\n",
    "    def __init__(self, path_to_dir_A, path_to_dir_B):\n",
    "        self.path_A = path_to_dir_A\n",
    "        self.path_B = path_to_dir_B\n",
    "        self.files_A = glob.glob1(self.path_A, '*.pkl')\n",
    "        self.files_B = glob.glob1(self.path_B, '*.pkl')\n",
    "        \n",
    "    def __len__(self):\n",
    "        len_A = len(glob.glob1(self.path_A, '*.pkl'))\n",
    "        len_B = len(glob.glob1(self.path_A, '*.pkl'))\n",
    "        assert len_A == len_B\n",
    "        return len_A\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = str(index).zfill(6) + '.pkl'\n",
    "        example_A = torch.load(f'{self.path_A}/{filename}')\n",
    "        example_B = torch.load(f'{self.path_B}/{filename}')\n",
    "        return example_A, example_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dir:  /home/ec2-user/SageMaker/train_data/rignet_testing_A\n",
      "train_dir:  /home/ec2-user/SageMaker/train_data/rignet_testing_B\n"
     ]
    }
   ],
   "source": [
    "train_dir_A = f'{base_path}/train_data/rignet_testing_A'\n",
    "print(\"train_dir: \", train_dir_A)\n",
    "train_dir_B = f'{base_path}/train_data/rignet_testing_B'\n",
    "print(\"train_dir: \", train_dir_B)\n",
    "dataset = DatasetSiamese(train_dir_A, train_dir_B)\n",
    "\n",
    "batch_size = 4\n",
    "sampler = None\n",
    "# # if args.distributed:\n",
    "# #     sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "# #         dataset,\n",
    "# #         num_replicas=torch.cuda.device_count(),\n",
    "# # # #             num_replicas=4,\n",
    "# # #             rank = args.local_rank,\n",
    "# #     )\n",
    "\n",
    "loader = data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=(sampler is None),\n",
    "    num_workers=0,\n",
    "#     collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    sampler=sampler,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "### TODO:\n",
    "### - Does not work. Looks for size() in Dataset as if it's a tensor.\n",
    "###   How to update to properly work?\n",
    "###\n",
    "# dataset = torch.utils.data.TensorDataset(dataset_A, dataset_B)\n",
    "# dataloader = data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "# # for index, (xb1, xb2) in enumerate(dataloader):\n",
    "\n",
    "\n",
    "# final_dataset = torch.utils.data.ConcatDataset((dataset_A, dataset_B))\n",
    "# loader = data.DataLoader(\n",
    "#     dataset=final_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=(sampler is None),\n",
    "#     num_workers=0,\n",
    "#     pin_memory=True,\n",
    "#     sampler=sampler,\n",
    "#     drop_last=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for A, B in loader:\n",
    "#     latents = A['latents'].cuda()\n",
    "#     landmarks_2d_gt = A['landmarks_2d_gt'].cuda()\n",
    "#     images = A['images'].cuda()\n",
    "#     image_masks = A['image_masks'].cuda()\n",
    "\n",
    "#     print(A.keys())\n",
    "#     print(B.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for A, B in loader:\n",
    "    w_A = A['latents'].cuda()\n",
    "    landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "    images_A = A['images'].cuda()\n",
    "    image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "    w_B = B['latents'].cuda()\n",
    "    landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "    images_B = B['images'].cuda()\n",
    "    image_masks_B = B['image_masks'].cuda()\n",
    "#     print(A.keys())\n",
    "#     print(B.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot conditional labels for supporting RigNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 6])\n",
      "torch.Size([1, 1, 6])\n",
      "labels_in:  tensor([[[0, 0, 0, 0, 1, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 1, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 1, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 1, 0]]], device='cuda:0')\n",
      "labels_pose:  tensor([[[0, 0, 0, 0, 1, 0]]])\n",
      "labels_expr:  tensor([[[0, 0, 1, 0, 0, 0]]])\n",
      "tensor([[[0, 0, 1, 0, 1, 0]]])\n"
     ]
    }
   ],
   "source": [
    "onehot_pose = F.one_hot(torch.tensor([4]), 6).unsqueeze(0)\n",
    "onehot_expr = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "\n",
    "# labels_in = F.one_hot(torch.tensor([4]), 6).unsqueeze(0).expand(4,-1,-1)\n",
    "# labels_in = labels_in.cuda()\n",
    "\n",
    "# labels_in = torch.cat((onehot_pose, onehot_expr), dim=0).cuda()\n",
    "\n",
    "labels_in = onehot_pose\n",
    "\n",
    "labels_in = labels_in.expand(batch_size, -1, -1)\n",
    "labels_in = labels_in.cuda()\n",
    "\n",
    "print(labels_in.shape)\n",
    "print(onehot_pose.shape)\n",
    "print(\"labels_in: \", labels_in)\n",
    "print(\"labels_pose: \", onehot_pose)\n",
    "print(\"labels_expr: \", onehot_expr)\n",
    "print(onehot_pose + onehot_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Calculate parameters from latents\n",
    "#\n",
    "# params_A = dfr(w_A.view(w_A.shape[0], -1))\n",
    "# params_B = dfr(w_B.view(w_B.shape[0], -1))\n",
    "\n",
    "params_A = dfr(nn.Flatten()(w_A))\n",
    "params_B = dfr(nn.Flatten()(w_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Transfer semantic \"style\" (e.g. pose) from one latent to another.\n",
    "#\n",
    "w_Ahat = rignet(w_B, params_A, labels_in) # Transfer semantic params from A to latent B\n",
    "w_Bhat = rignet(w_A, params_B, labels_in) # Transfer semantic params from B to latent A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create the cycle, where transferred paramter to latent, and latent to parameter\n",
    "#    should match.\n",
    "#\n",
    "params_AB = dfr(nn.Flatten()(w_Ahat))\n",
    "params_BA = dfr(nn.Flatten()(w_Bhat))\n",
    "\n",
    "# shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "params_edit_A = [x.clone() for x in params_A]\n",
    "params_edit_A[2] = params_AB[2] # Ensure pose is maintained\n",
    "\n",
    "params_edit_B = [x.clone() for x in params_B]\n",
    "params_edit_B[2] = params_BA[2] # Ensure pose is maintained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the rendering and return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_editing_loss(example, param, _flame, _flametex):\n",
    "    import util\n",
    "    \n",
    "    loss_mse = nn.MSELoss().cuda()\n",
    "    \n",
    "#     w_A = A['latents'].cuda()\n",
    "#     landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "#     images_A = A['images'].cuda()\n",
    "#     image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "#     w_B = B['latents'].cuda()\n",
    "#     landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "#     images_B = B['images'].cuda()\n",
    "#     image_masks_B = B['image_masks'].cuda()\n",
    "\n",
    "    latents = example['latents'].cuda()\n",
    "    landmarks_2d_gt = example['landmarks_2d_gt'].cuda()\n",
    "    landmarks_3d_gt = example['landmarks_3d_gt'].cuda()\n",
    "    images = example['images'].cuda()\n",
    "    image_masks = example['image_masks'].cuda()\n",
    "    \n",
    "    \n",
    "    # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "    shape, expression, pose, tex, cam, lights = param\n",
    "    vertices, landmarks2d, landmarks3d = flame(shape_params=shape,\n",
    "                                               expression_params=expression,\n",
    "                                               pose_params=pose)\n",
    "\n",
    "    # render\n",
    "    #\n",
    "    albedos = flametex(tex) / 255.\n",
    "    \n",
    "    trans_vertices = util.batch_orth_proj(vertices, cam);\n",
    "    trans_vertices[..., 1:] = - trans_vertices[..., 1:]\n",
    "    landmarks2d = util.batch_orth_proj(landmarks2d, cam);\n",
    "    landmarks2d[..., 1:] = - landmarks2d[..., 1:]\n",
    "    landmarks3d = util.batch_orth_proj(landmarks3d, cam);\n",
    "    landmarks3d[..., 1:] = - landmarks3d[..., 1:]\n",
    "    \n",
    "    losses = {}\n",
    "    losses['landmark_2d'] = util.l2_distance(landmarks2d[:, :, :2],\n",
    "                                              landmarks_2d_gt[:, :, :2]) * 1.0\n",
    "    losses['landmark_3d'] = util.l2_distance(landmarks3d[:, :, :2],\n",
    "                                              landmarks_3d_gt[:, :, :2]) * 1.0\n",
    "    losses['shape_reg'] = (torch.sum(shape ** 2) / 2) * 1e-4\n",
    "    losses['expression_reg'] = (torch.sum(expression ** 2) / 2) * 1e-4\n",
    "    losses['pose_reg'] = (torch.sum(pose ** 2) / 2) * 1e-4 #config.w_pose_reg\n",
    "\n",
    "#     # Regularize learned texture.\n",
    "#     #\n",
    "#     losses['texture_reg'] = loss_mse(albedos, texture_mean.repeat(args.batch_size, 1, 1, 1)) \n",
    "    \n",
    "    \n",
    "    ## render\n",
    "    albedos = flametex(tex) / 255.\n",
    "    ops = render(vertices, trans_vertices, albedos, lights)\n",
    "\n",
    "    predicted_images = ops['images']\n",
    "    losses['photometric_texture'] = (image_masks * (predicted_images - images).abs()).mean() \\\n",
    "                                    * 1.0 #config.w_pho\n",
    "\n",
    "\n",
    "    all_loss = 0.\n",
    "    for key in losses.keys():\n",
    "        all_loss = all_loss + losses[key]\n",
    "#         losses_to_plot[key].append(losses[key].item()) # Store for plotting later.\n",
    "\n",
    "\n",
    "    losses['all_loss'] = all_loss\n",
    "#     losses_to_plot['all_loss'].append(losses['all_loss'].item())\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_A = cycle_editing_loss(A, params_edit_A, flame, flametex)\n",
    "loss_B = cycle_editing_loss(B, params_edit_B, flame, flametex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(\n",
    "    rignet.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.0001 # config.e_wd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = loss_A['all_loss'] + loss_B['all_loss']\n",
    "\n",
    "optim.zero_grad()\n",
    "total_loss.backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2299, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RigNet model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "del rignet\n",
    "\n",
    "# one_hot = True\n",
    "one_hot = False\n",
    "rignet = load.rignet(base_path, one_hot=one_hot).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim = torch.optim.Adam(\n",
    "#     rignet.parameters(),\n",
    "#     lr=1e-3,    \n",
    "# #     weight_decay=0.00001 # config.e_wd\n",
    "# )\n",
    "\n",
    "optim = Adamax(rignet.parameters()) # default Adamax # Trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , loss_edit:  2.1838061809539795 , loss_recon:  0.0013645573053508997\n",
      "epoch:  100 , loss_edit:  2.1868162155151367 , loss_recon:  0.0008672431576997042\n",
      "epoch:  200 , loss_edit:  2.173971652984619 , loss_recon:  0.0018171978881582618\n",
      "epoch:  300 , loss_edit:  2.2019596099853516 , loss_recon:  0.0010616485960781574\n",
      "epoch:  400 , loss_edit:  2.199246883392334 , loss_recon:  0.0007042901124805212\n",
      "epoch:  500 , loss_edit:  2.1893868446350098 , loss_recon:  0.00011368238483555615\n",
      "epoch:  600 , loss_edit:  2.199608564376831 , loss_recon:  8.94882577995304e-06\n",
      "epoch:  700 , loss_edit:  2.1680824756622314 , loss_recon:  2.39634368881525e-06\n",
      "epoch:  800 , loss_edit:  2.2097766399383545 , loss_recon:  2.2328259774440085e-07\n",
      "epoch:  900 , loss_edit:  2.176872968673706 , loss_recon:  5.433705609902972e-07\n",
      "epoch:  1000 , loss_edit:  2.20977520942688 , loss_recon:  2.4079341187643877e-07\n",
      "epoch:  1100 , loss_edit:  2.1632168292999268 , loss_recon:  1.0533121894695796e-06\n",
      "epoch:  1200 , loss_edit:  2.2170326709747314 , loss_recon:  2.567528554209275e-07\n",
      "epoch:  1300 , loss_edit:  2.1882359981536865 , loss_recon:  1.0773376288852887e-06\n",
      "epoch:  1400 , loss_edit:  2.1632168292999268 , loss_recon:  2.9997204364917707e-06\n",
      "epoch:  1500 , loss_edit:  2.176862955093384 , loss_recon:  2.735929228947498e-06\n",
      "epoch:  1600 , loss_edit:  2.18554425239563 , loss_recon:  1.473401312068745e-07\n",
      "epoch:  1700 , loss_edit:  2.199603796005249 , loss_recon:  1.3288867194205523e-05\n",
      "epoch:  1800 , loss_edit:  2.1853795051574707 , loss_recon:  1.1173093525940203e-06\n",
      "epoch:  1900 , loss_edit:  2.1605565547943115 , loss_recon:  2.985714786518656e-07\n",
      "epoch:  2000 , loss_edit:  2.2211575508117676 , loss_recon:  5.259483259578701e-06\n",
      "epoch:  2100 , loss_edit:  2.1750426292419434 , loss_recon:  1.4209886103344616e-06\n",
      "epoch:  2200 , loss_edit:  2.209604024887085 , loss_recon:  1.510033698082225e-08\n",
      "epoch:  2300 , loss_edit:  2.17781925201416 , loss_recon:  1.0320113688067067e-06\n",
      "epoch:  2400 , loss_edit:  2.1745779514312744 , loss_recon:  6.34606749372324e-08\n",
      "epoch:  2500 , loss_edit:  2.170767307281494 , loss_recon:  5.901374322547781e-08\n",
      "epoch:  2600 , loss_edit:  2.179447650909424 , loss_recon:  1.4711415587953525e-06\n",
      "epoch:  2700 , loss_edit:  2.1819353103637695 , loss_recon:  4.002046807727311e-06\n",
      "epoch:  2800 , loss_edit:  2.174588203430176 , loss_recon:  6.287698397500208e-06\n",
      "epoch:  2900 , loss_edit:  2.159395456314087 , loss_recon:  2.2951951450522756e-07\n",
      "epoch:  3000 , loss_edit:  2.1993985176086426 , loss_recon:  5.773923561491756e-08\n",
      "epoch:  3100 , loss_edit:  2.203672170639038 , loss_recon:  1.979571839783034e-09\n",
      "epoch:  3200 , loss_edit:  2.184782028198242 , loss_recon:  3.980924923041584e-09\n",
      "epoch:  3300 , loss_edit:  2.159400463104248 , loss_recon:  3.151437340420671e-06\n",
      "epoch:  3400 , loss_edit:  2.185544490814209 , loss_recon:  5.085207703814376e-07\n",
      "epoch:  3500 , loss_edit:  2.1995596885681152 , loss_recon:  6.810782906541135e-07\n",
      "epoch:  3600 , loss_edit:  2.182300090789795 , loss_recon:  3.4093972089976887e-07\n",
      "epoch:  3700 , loss_edit:  2.1709396839141846 , loss_recon:  1.8910964172391687e-06\n",
      "epoch:  3800 , loss_edit:  2.185389518737793 , loss_recon:  5.344660166883841e-06\n",
      "epoch:  3900 , loss_edit:  2.1778247356414795 , loss_recon:  3.161517724947771e-06\n",
      "epoch:  4000 , loss_edit:  2.2211437225341797 , loss_recon:  1.3386924138103495e-06\n",
      "epoch:  4100 , loss_edit:  2.170936107635498 , loss_recon:  1.5297956679205527e-06\n",
      "epoch:  4200 , loss_edit:  2.1925125122070312 , loss_recon:  5.945969405729556e-07\n",
      "epoch:  4300 , loss_edit:  2.1882359981536865 , loss_recon:  2.8159297471574973e-06\n",
      "epoch:  4400 , loss_edit:  2.193467378616333 , loss_recon:  7.89590330896317e-08\n",
      "epoch:  4500 , loss_edit:  2.1820948123931885 , loss_recon:  4.159079747267924e-10\n",
      "epoch:  4600 , loss_edit:  2.1981379985809326 , loss_recon:  1.5199284053579731e-09\n",
      "epoch:  4700 , loss_edit:  2.207134962081909 , loss_recon:  8.780304597166833e-06\n",
      "epoch:  4800 , loss_edit:  2.203671932220459 , loss_recon:  6.32297769698198e-08\n",
      "epoch:  4900 , loss_edit:  2.1806695461273193 , loss_recon:  1.002518388304452e-06\n",
      "epoch:  5000 , loss_edit:  2.1707653999328613 , loss_recon:  2.3615339159732684e-06\n",
      "epoch:  5100 , loss_edit:  2.160722255706787 , loss_recon:  5.748200493371769e-08\n",
      "epoch:  5200 , loss_edit:  2.1707608699798584 , loss_recon:  3.120781286725105e-07\n",
      "epoch:  5300 , loss_edit:  2.192052125930786 , loss_recon:  8.700037142261863e-06\n",
      "epoch:  5400 , loss_edit:  2.184781789779663 , loss_recon:  2.0198688943651177e-08\n",
      "epoch:  5500 , loss_edit:  2.1880276203155518 , loss_recon:  1.397629034727288e-06\n",
      "epoch:  5600 , loss_edit:  2.1781210899353027 , loss_recon:  1.2595912721735658e-06\n",
      "epoch:  5700 , loss_edit:  2.174576997756958 , loss_recon:  1.2714295394289366e-07\n",
      "epoch:  5800 , loss_edit:  2.2170283794403076 , loss_recon:  8.821215757848222e-09\n",
      "epoch:  5900 , loss_edit:  2.1811418533325195 , loss_recon:  2.6508064365771133e-06\n",
      "epoch:  6000 , loss_edit:  2.177840232849121 , loss_recon:  1.0566769560682587e-05\n",
      "epoch:  6100 , loss_edit:  2.2211427688598633 , loss_recon:  5.020577305003826e-07\n",
      "epoch:  6200 , loss_edit:  2.210930585861206 , loss_recon:  3.3295819434897567e-08\n",
      "epoch:  6300 , loss_edit:  2.1704657077789307 , loss_recon:  2.3292659534490667e-06\n",
      "epoch:  6400 , loss_edit:  2.1893534660339355 , loss_recon:  1.9519583815963415e-07\n",
      "epoch:  6500 , loss_edit:  2.1981403827667236 , loss_recon:  1.3820331332681235e-06\n",
      "epoch:  6600 , loss_edit:  2.170927047729492 , loss_recon:  4.1751412993562553e-08\n",
      "epoch:  6700 , loss_edit:  2.1891930103302 , loss_recon:  8.825526833788899e-07\n",
      "epoch:  6800 , loss_edit:  2.18965744972229 , loss_recon:  6.904572913413176e-09\n",
      "epoch:  6900 , loss_edit:  2.1704628467559814 , loss_recon:  4.072248371844722e-10\n",
      "epoch:  7000 , loss_edit:  2.188399076461792 , loss_recon:  1.549936087030801e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = int(10e3)\n",
    "\n",
    "\n",
    "# shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "# onehot_cam = F.one_hot(torch.tensor([4]), 6).unsqueeze(0)\n",
    "onehot_pose = F.one_hot(torch.tensor([2]), 6).unsqueeze(0)\n",
    "\n",
    "\n",
    "# labels_in = torch.cat((onehot_pose, onehot_expr), dim=0).cuda()\n",
    "labels_in = onehot_pose\n",
    "labels_in = labels_in.expand(batch_size, -1, -1)\n",
    "labels_in = labels_in.cuda()\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    for A, B in loader:\n",
    "        w_A = A['latents'].cuda()\n",
    "        landmarks_2d_gt_A = A['landmarks_2d_gt'].cuda()\n",
    "        images_A = A['images'].cuda()\n",
    "        image_masks_A = A['image_masks'].cuda()\n",
    "\n",
    "        w_B = B['latents'].cuda()\n",
    "        landmarks_2d_gt_B = B['landmarks_2d_gt'].cuda()\n",
    "        images_B = B['images'].cuda()\n",
    "        image_masks_B = B['image_masks'].cuda()\n",
    "\n",
    "\n",
    "        # 1) Calculate parameters from latents\n",
    "        #\n",
    "        # params_A = dfr(w_A.view(w_A.shape[0], -1))\n",
    "        # params_B = dfr(w_B.view(w_B.shape[0], -1))\n",
    "        params_A = dfr(nn.Flatten()(w_A))\n",
    "        params_B = dfr(nn.Flatten()(w_B))\n",
    "        \n",
    "        \n",
    "        # 2) Transfer semantic \"style\" (e.g. pose) from one latent to another.\n",
    "        #\n",
    "        w_Ahat = rignet(w_B, params_A, labels_in) # Transfer semantic params from A to latent B\n",
    "        w_Bhat = rignet(w_A, params_B, labels_in) # Transfer semantic params from B to latent A\n",
    "\n",
    "        \n",
    "        # 3) Create the cycle, where transferred paramter to latent, and latent to parameter\n",
    "        #    should match.\n",
    "        #\n",
    "        params_AB = dfr(nn.Flatten()(w_Ahat))\n",
    "        params_BA = dfr(nn.Flatten()(w_Bhat))\n",
    "\n",
    "        \n",
    "        # shape, expression, pose, tex, cam, lights = dfr(latents.view(args.batch_size, -1))\n",
    "        params_edit_A = [x.clone() for x in params_A]\n",
    "        params_edit_A[2] = params_AB[2] # Ensure pose is maintained\n",
    "        params_edit_B = [x.clone() for x in params_B]\n",
    "        params_edit_B[2] = params_BA[2] # Ensure pose is maintained\n",
    "\n",
    "\n",
    "        # Losses\n",
    "        # ------------------------------------------------\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        \n",
    "        # Latent reconstruction loss.\n",
    "        #\n",
    "        scale_recon = 0.1\n",
    "        loss_recon = 0.0\n",
    "        w_hat = rignet(w_A, params_A, labels_in) # Reconsruct self\n",
    "        loss_recon = (w_hat - w_A).abs().square().mean() * scale_recon\n",
    "        w_hat = rignet(w_B, params_B, labels_in) # Reconsruct self\n",
    "        loss_recon += (w_hat - w_B).abs().square().mean() * scale_recon\n",
    "        \n",
    "        \n",
    "        loss_A = cycle_editing_loss(A, params_edit_A, flame, flametex)\n",
    "        loss_B = cycle_editing_loss(B, params_edit_B, flame, flametex)\n",
    "#         loss_A = cycle_editing_loss(A, params_edit_B, flame, flametex)\n",
    "#         loss_B = cycle_editing_loss(B, params_edit_A, flame, flametex)\n",
    "        loss_edit = loss_A['all_loss'] + loss_B['all_loss']\n",
    "#         loss_edit = loss_A['all_loss']\n",
    "        \n",
    "    \n",
    "        total_loss += loss_recon\n",
    "        total_loss += loss_edit\n",
    "        \n",
    "        \n",
    "        optim.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"epoch: \", epoch, \", loss_edit: \", total_loss.item(), \", loss_recon: \", loss_recon.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.RigNet import StyleEncoder, StyleDecoder, RigNet\n",
    "# import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims_enc_in=(18,512)\n",
    "# dims_enc_out=(18,32) \n",
    "# one_hot=True\n",
    "# num_classes=6\n",
    "\n",
    "# enc = StyleEncoder(dims_enc_in, dims_enc_out, one_hot, num_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims_dec_in = (18, 268)\n",
    "# dims_dec_out=(18, 512)\n",
    "\n",
    "# if one_hot:\n",
    "#     dims_dec_in = (dims_dec_in[0], dims_dec_in[-1] + num_classes)\n",
    "\n",
    "# del dec\n",
    "# dec = StyleDecoder(dims_dec_in, dims_dec_out).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 18, 38])\n"
     ]
    }
   ],
   "source": [
    "# x = enc(w_B, labels_in)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_flat:  torch.Size([4, 18, 236])\n",
      "w_enc:  torch.Size([4, 18, 38])\n",
      "params_flat:  torch.Size([4, 18, 236])\n",
      "x:  torch.Size([4, 18, 274])\n",
      "dims_in:  (18, 274)\n"
     ]
    }
   ],
   "source": [
    "# x = dec(x, params_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RigNet model...ONE_HOT\n"
     ]
    }
   ],
   "source": [
    "# rignet = load.rignet(base_path, one_hot=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_in = F.one_hot(torch.tensor([4]), 6).unsqueeze(0).expand(4,-1,-1)\n",
    "# labels_in = labels_in.cuda()\n",
    "\n",
    "\n",
    "# print(labels_in.shape)\n",
    "# print(w_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_Ahat = rignet(w_B, params_A, labels_in) # Transfer semantic params from A to latent B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2) Transfer semantic \"style\" (e.g. pose) from one latent to another.\n",
    "# #\n",
    "# w_Ahat = rignet(w_B, params_A) # Transfer semantic params from A to latent B\n",
    "# w_Bhat = rignet(w_A, params_B) # Transfer semantic params from B to latent A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom (my_pytorch17_py36)",
   "language": "python",
   "name": "my_pytorch17_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
